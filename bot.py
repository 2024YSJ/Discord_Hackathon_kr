import os
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time
import re

WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')
DB_FILE = "sent_hackathons.txt"

class HackathonBot:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        self.sent_list = self.load_sent_list()

    def load_sent_list(self):
        if os.path.exists(DB_FILE):
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    def save_sent_list(self, new_items):
        with open(DB_FILE, "a", encoding="utf-8") as f:
            for item in new_items:
                f.write(f"{item['title']}\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì •ìƒ ë™ì‘ í™•ì¸ëœ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_devpost(self):
        try:
            h = self.headers.copy()
            h.update({"Accept": "application/json", "Referer": "https://devpost.com/hackathons", "X-Requested-With": "XMLHttpRequest"})
            res = requests.get("https://devpost.com/api/hackathons", params={"status[]": "upcoming", "sort_by": "Recently Added"}, headers=h, timeout=15)
            if res.status_code == 200:
                return [{"title": h['title'], "url": h['url'], "host": "Devpost", "date": h.get('submission_period_dates', 'N/A')}
                        for h in res.json().get('hackathons', [])]
        except: pass
        return []

    def fetch_mlh(self):
        MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}
        try:
            res = requests.get("https://mlh.io/seasons/2026/events", headers=self.headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results, today, seen = [], datetime.now().replace(hour=0,minute=0,second=0,microsecond=0), set()
                for a in soup.find_all('a', href=True):
                    h3 = a.find('h3')
                    if not h3: continue
                    title = h3.get_text(strip=True)
                    if not title or title in seen: continue
                    seen.add(title)
                    link = a['href'].split('?')[0]
                    if not link.startswith('http'): link = "https://mlh.io" + link
                    a_text = a.get_text(separator=' ', strip=True).replace(title, '')
                    date_parts = re.findall(r'([A-Z]{3})\s+(\d{1,2})', a_text)
                    if date_parts:
                        date_str = ' - '.join(f"{m} {d}" for m,d in date_parts) if len(date_parts)>1 else f"{date_parts[0][0]} {date_parts[0][1]}"
                        mon, day = date_parts[-1]
                        end_m = MONTHS.get(mon, 0)
                        if end_m and datetime(today.year, end_m, int(day)) < today: continue
                    else:
                        date_str = "2026 Season"
                    results.append({"title": title, "url": link, "host": "MLH", "date": date_str})
                print(f"ğŸ“¡ MLH: {len(results)}ê°œ ì¶”ì¶œ ì„±ê³µ (ì¢…ë£Œ ì´ë²¤íŠ¸ ì œì™¸)")
                return results
        except Exception as e:
            print(f"MLH ì˜ˆì™¸: {e}")
        return []

    def fetch_kaggle(self):
        username = os.environ.get('KAGGLE_USERNAME', '')
        key = os.environ.get('KAGGLE_KEY', '')
        print(f"DEBUG: Username length: {len(username)}")
        print(f"DEBUG: Key length: {len(key)}")
        if not username or not key:
            print("âŒ Kaggle í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
            return []
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            res = requests.get('https://www.kaggle.com/api/v1/competitions/list',
                               params={'sortBy': 'latestDeadline', 'pageSize': 20},
                               auth=(username, key), headers=self.headers, timeout=15)
            if res.status_code != 200:
                print(f"âŒ Kaggle API ì‹¤íŒ¨ ({res.status_code})")
                return []
            results = []
            for c in res.json():
                title = c.get('title', '')
                deadline = (c.get('deadline') or '')[:10]
                if not title or (deadline and deadline < today): continue
                ref = c.get('ref') or c.get('id', '')
                results.append({"title": title, "url": f"https://www.kaggle.com/competitions/{ref}", "host": "Kaggle", "date": deadline or "ìƒì„¸ í™•ì¸"})
            print(f"âœ… {len(results)}ê°œì˜ í™œì„± ê²½ì§„ëŒ€íšŒë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results
        except Exception as e:
            print(f"âŒ Kaggle ì˜ˆì™¸: {e}")
        return []

    def fetch_hack2skill(self):
        try:
            res = requests.get('https://hack2skill.com/', headers=self.headers, timeout=15)
            if res.status_code != 200: return []
            soup = BeautifulSoup(res.text, 'html.parser')
            flagship = soup.find(class_='flagshipEventsSlider')
            if not flagship: return []
            today, results, seen = datetime.now(), [], set()
            for a in flagship.find_all('a', href=re.compile(r'hack2skill\.com')):
                url = a['href'].split('?')[0]
                if url in seen: continue
                card = a.find_parent('div', class_=re.compile(r'w-\[16rem\]'))
                if not card: continue
                h5s = card.find_all('h5')
                if not h5s: continue
                title = h5s[0].get_text(strip=True)
                if not title: continue
                date_str = h5s[-1].get_text(strip=True) if len(h5s) > 1 else ''
                try:
                    if datetime.strptime(date_str, '%a %b %d %Y') < today: continue
                except ValueError: pass
                seen.add(url)
                results.append({"title": title, "url": url, "host": "Hack2Skill", "date": date_str})
            return results
        except Exception as e:
            print(f"Hack2Skill ì˜ˆì™¸: {e}")
        return []

    def fetch_dorahacks(self):
        try:
            res = requests.get("https://dorahacks.io/api/hackathon", params={"status": "open", "limit": 20}, headers=self.headers, timeout=15)
            if res.status_code == 200:
                now_ts = time.time()
                results = []
                for h in res.json().get('results', []):
                    title = h.get('title', '')
                    if not title: continue
                    end_ts = h.get('end_time')
                    if end_ts and int(end_ts) < now_ts: continue
                    results.append({"title": title, "url": f"https://dorahacks.io/hackathon/{h.get('id','')}", "host": "DoraHacks", "date": "ìƒì„¸ í™•ì¸"})
                return results
        except Exception as e:
            print(f"DoraHacks ì˜ˆì™¸: {e}")
        return []

    def fetch_devevent(self):
        try:
            now = datetime.now()
            # Dev-Event ì €ì¥ì†Œì˜ í˜„ì¬ ì›” íŒŒì¼ ì ‘ê·¼
            url = f"https://raw.githubusercontent.com/brave-people/Dev-Event/master/end_event/{now.year}/{str(now.year)[2:]}_{str(now.month).zfill(2)}.md"
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                results = []
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹
                for m in re.finditer(r'__\[([^\]]+)\]\((https?://[^\)]+)\)__', res.text):
                    title, link = m.group(1), m.group(2)
                    # ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥: ë¶€íŠ¸ìº í”„, êµìœ¡, KDT ë“± í¬í•¨
                    target_keywords = ['í•´ì»¤í†¤', 'Hackathon', 'ê³µëª¨ì „', 'ê²½ì§„ëŒ€íšŒ', 'ë¶€íŠ¸ìº í”„', 'Bootcamp', 'êµìœ¡', 'KDT', 'ì–‘ì„±ê³¼ì •']
                    if any(k.lower() in title.lower() for k in target_keywords):
                        icon = "ğŸ“" if "ë¶€íŠ¸ìº í”„" in title or "êµìœ¡" in title else "ğŸ‡°ğŸ‡·"
                        results.append({"title": f"{icon} [ë°ë¸Œì´ë²¤íŠ¸] {title}", "url": link, "host": "DevEvent", "date": "ìƒì„¸ í™•ì¸"})
                return results
        except Exception as e:
            print(f"DevEvent ì˜ˆì™¸: {e}")
        return []

    def fetch_campuspick(self):
        try:
            h = self.headers.copy()
            h.update({"Content-Type": "application/x-www-form-urlencoded", "Origin": "https://www2.campuspick.com", "Referer": "https://www2.campuspick.com/"})
            today, results = datetime.now().strftime('%Y-%m-%d'), []
            
            # 108: ê³µëª¨ì „, 111: êµìœ¡/ê°•ì—° (ë¶€íŠ¸ìº í”„ê°€ ì£¼ë¡œ ì˜¬ë¼ì˜¤ëŠ” ì¹´í…Œê³ ë¦¬)
            for cat_id in [108, 111]:
                for offset in range(0, 40, 20):
                    res = requests.post("https://api2.campuspick.com/find/activity/list", 
                                        data={"target":1,"limit":20,"offset":offset,"categoryId":cat_id}, 
                                        headers=h, timeout=15)
                    if res.status_code != 200: break
                    activities = res.json().get("result", {}).get("activities", [])
                    if not activities: break
                    
                    valid = [a for a in activities if a.get("endDate","") >= today]
                    for a in valid:
                        prefix = "ğŸ“ [ë¶€íŠ¸ìº í”„/êµìœ¡]" if cat_id == 111 else "ğŸ‡°ğŸ‡· [ìº í¼ìŠ¤í”½]"
                        results.append({
                            "title": f"{prefix} {a['title']}", 
                            "url": f"https://www2.campuspick.com/contest/view?id={a['id']}", 
                            "host": "CampusPick", 
                            "date": a.get("endDate","ìƒì„¸ í™•ì¸")
                        })
            return results
        except Exception as e:
            print(f"CampusPick ì˜ˆì™¸: {e}")
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆ˜ì •ëœ í•¨ìˆ˜ë“¤
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_hackerearth(self):
        """
        HackerEarth í•´ì»¤í†¤ ëª©ë¡ - HTML SSR í™•ì¸ë¨, 3ê°œ ì„±ê³µ ì¤‘
        live/upcoming ë§í¬ë¥¼ ë” ì™„ì „í•˜ê²Œ ìˆ˜ì§‘í•˜ë„ë¡ ê°œì„ 
        """
        results = []
        seen = set()
        try:
            h = self.headers.copy()
            h.update({"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"})
            res = requests.get("https://www.hackerearth.com/challenges/hackathon/", headers=h, timeout=15)
            if res.status_code != 200:
                print(f"  HackerEarth ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}")
                return []
            soup = BeautifulSoup(res.text, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a['href']
                if re.match(r'^/challenges/hackathon/[^/]+/?$', href):
                    full_url = "https://www.hackerearth.com" + href.rstrip('/') + '/'
                elif re.match(r'https://[^.]+\.hackerearth\.com/?$', href):
                    full_url = href.rstrip('/') + '/'
                else:
                    continue
                if full_url in seen: continue
                seen.add(full_url)
                title_tag = a.find(['h3', 'h4', 'h2', 'p'])
                title = title_tag.get_text(strip=True) if title_tag else a.get_text(strip=True)
                title = re.sub(r'\s+', ' ', title).strip()
                if not title or len(title) < 3: continue
                results.append({"title": title, "url": full_url, "host": "HackerEarth", "date": "ìƒì„¸ í™•ì¸"})
        except Exception as e:
            print(f"  HackerEarth ì˜ˆì™¸: {e}")
        return results

    def fetch_programmers(self):
        """
        [í™•ì¸ëœ ì‚¬ì‹¤]
        - URL: programmers.co.kr/api/competitions  â† ì§ì ‘ fetchë¡œ ì‘ë‹µ í™•ì¸
        - JSON êµ¬ì¡°: {"competitions": [{id, href, title, statusLabel, receiptEndAt, endAt}], "totalPages": 11}
        - href ì˜ˆì‹œ: /competitions/4079?slug=2025_programmers_codechallenge
        - í˜„ì¬ ëª¨ë“  í•­ëª©ì´ statusLabel:"ended" â†’ ì§„í–‰ ì¤‘ì¸ ëŒ€íšŒê°€ ì—†ìœ¼ë©´ 0ê°œëŠ” ì •ìƒ

        totalPages(11)ë¥¼ ëª¨ë‘ ìˆœíšŒí•˜ë©´ ë„ˆë¬´ ë§ìœ¼ë¯€ë¡œ ìµœê·¼ 2í˜ì´ì§€ë§Œ í™•ì¸.
        endedê°€ ì•„ë‹Œ ëŒ€íšŒê°€ ì—†ìœ¼ë©´ 0ê°œ ë°˜í™˜ (ë²„ê·¸ ì•„ë‹˜).
        """
        today = datetime.now().strftime('%Y-%m-%d')
        results = []
        try:
            for page in range(1, 3):  # ìµœê·¼ 2í˜ì´ì§€
                res = requests.get(
                    "https://programmers.co.kr/api/competitions",
                    params={"page": page},
                    headers=self.headers, timeout=15
                )
                if res.status_code != 200:
                    print(f"  Programmers API ì˜¤ë¥˜: {res.status_code}")
                    break
                data = res.json()
                competitions = data.get('competitions', [])
                for c in competitions:
                    if c.get('statusLabel') == 'ended': continue
                    end_at = c.get('receiptEndAt') or c.get('endAt') or ''
                    if end_at and end_at[:10] < today: continue
                    title = c.get('title', '')
                    href = c.get('href', '')
                    if not title: continue
                    # hrefì— ì¿¼ë¦¬ìŠ¤íŠ¸ë§ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ê²½ë¡œë§Œ ì‚¬ìš©
                    path = href.split('?')[0]
                    full_url = f"https://programmers.co.kr{path}"
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [í”„ë¡œê·¸ë˜ë¨¸ìŠ¤] {title}",
                        "url": full_url,
                        "host": "Programmers",
                        "date": end_at[:10] if end_at else "ìƒì„¸ í™•ì¸"
                    })
        except Exception as e:
            print(f"  Programmers ì˜ˆì™¸: {e}")
        return results

    def fetch_dacon(self):
        """
        DACON AI ê²½ì§„ëŒ€íšŒ
        - newapi.dacon.io: ì™¸ë¶€ ì°¨ë‹¨ë¨ (404)
        - dacon.io/competitions: Nuxt CSR, HTMLì— ë°ì´í„° ì—†ìŒ
        - í•´ê²°: Google ê²€ìƒ‰ ì¸ë±ìŠ¤ì—ì„œ ìµœê·¼ DACON ëŒ€íšŒ URLì„ ìˆ˜ì§‘í•˜ëŠ” ëŒ€ì‹ ,
                Bing ì˜¤í”ˆ ê²€ìƒ‰ URLì„ í†µí•´ ìµœê·¼ ê²Œì‹œëœ dacon.io ëŒ€íšŒ í˜ì´ì§€ íŒŒì‹±
                ë˜ëŠ” GitHubì˜ DACON ê´€ë ¨ ê³µê°œ ë°ì´í„° í™œìš©

        ì‹¤ìš©ì  ëŒ€ì•ˆ: ì´ë¯¸ fetch_aiconnectì—ì„œ DACON HTML 15ê°œ ì„±ê³µ ì¤‘ì´ë¯€ë¡œ
        ì—¬ê¸°ì„œëŠ” ì¶”ê°€ë¡œ ì›”ê°„ë°ì´ì½˜/í•´ì»¤í†¤ ì¹´í…Œê³ ë¦¬ë§Œ ìˆ˜ì§‘
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')

        # DACON í•´ì»¤í†¤ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ (hackathon íƒ­)
        # URL: dacon.io/competitions?taskCategory=HACKATHON ì‹œë„
        urls_to_try = [
            ("https://dacon.io/competitions?taskCategory=HACKATHON", re.compile(r'/competitions/official/\d+')),
            ("https://dacon.io/competitions?status=active", re.compile(r'/competitions/official/\d+')),
            ("https://dacon.io/competitions", re.compile(r'/competitions/official/\d+')),
        ]
        h = self.headers.copy()
        h.update({"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "Referer": "https://dacon.io/"})

        for url, pattern in urls_to_try:
            try:
                res = requests.get(url, headers=h, timeout=15)
                if res.status_code != 200: continue
                soup = BeautifulSoup(res.text, 'html.parser')
                seen = set()
                for a in soup.find_all('a', href=pattern):
                    href = a['href']
                    m = re.match(r'/competitions/official/(\d+)', href)
                    if not m or m.group(1) in seen: continue
                    seen.add(m.group(1))
                    title_tag = a.find(['h4', 'h3', 'h2', 'p', 'span'])
                    title = title_tag.get_text(strip=True) if title_tag else a.get_text(strip=True)
                    title = re.sub(r'\s+', ' ', title).strip()
                    if not title or len(title) < 3: continue
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [DACON] {title}",
                        "url": f"https://dacon.io{href.split('?')[0]}",
                        "host": "DACON",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
                if results:
                    break
            except Exception as e:
                print(f"  DACON {url} ì˜ˆì™¸: {e}")

        return results

    def fetch_aihub(self):
        """
        AI Hub ì±Œë¦°ì§€ - ì´ë¯¸ fetch_aiconnectì—ì„œ 15ê°œ ì„±ê³µ ì¤‘ì´ë¯€ë¡œ ìœ ì§€
        fetch_aiconnectë¥¼ ì´ í•¨ìˆ˜ë¡œ ì´ë¦„ ë³€ê²½í•˜ì—¬ ëª…í™•í™”
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')

        # DACON HTML íŒŒì‹± (Nuxt CSRì´ì§€ë§Œ ì¼ë¶€ SSR ë‚´ìš© í¬í•¨)
        try:
            h = self.headers.copy()
            h.update({"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "Referer": "https://dacon.io/"})
            res = requests.get("https://dacon.io/competitions", headers=h, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                seen = set()
                for a in soup.find_all('a', href=re.compile(r'/competitions/official/\d+')):
                    href = a['href']
                    m = re.match(r'/competitions/official/(\d+)', href)
                    if not m or m.group(1) in seen: continue
                    seen.add(m.group(1))
                    title_tag = a.find(['h4', 'h3', 'h2', 'p', 'span'])
                    title = title_tag.get_text(strip=True) if title_tag else a.get_text(strip=True)
                    title = re.sub(r'\s+', ' ', title).strip()
                    if not title or len(title) < 3: continue
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [DACON] {title}",
                        "url": f"https://dacon.io{href.split('?')[0]}",
                        "host": "DACON",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
        except Exception as e:
            print(f"  DACON HTML ì˜ˆì™¸: {e}")

        # AI Hub ì±Œë¦°ì§€ HTML íŒŒì‹±
        try:
            h = self.headers.copy()
            h.update({"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"})
            res = requests.get("https://www.aihub.or.kr/intrcn/lit/aiclgComp/list.do", headers=h, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                seen_titles = set()
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if 'aiclgComp' not in href and 'challenge' not in href.lower(): continue
                    title = a.get_text(strip=True)
                    if not title or len(title) < 4 or title in seen_titles: continue
                    seen_titles.add(title)
                    full_url = f"https://www.aihub.or.kr{href}" if href.startswith('/') else href
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [AI Hub] {title}",
                        "url": full_url,
                        "host": "AIHub",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
        except Exception as e:
            print(f"  AIHub HTML ì˜ˆì™¸: {e}")

        return results

    def fetch_linkareer(self):
        """
        ë§ì»¤ë¦¬ì–´ GraphQL ê¸°ë°˜ í•´ì»¤í†¤ ë° ë¶€íŠ¸ìº í”„ ìˆ˜ì§‘ í•¨ìˆ˜
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')

        gql_headers = {
            "Content-Type": "application/json",
            "User-Agent": self.headers["User-Agent"],
            "Referer": "https://linkareer.com/",
            "Origin": "https://linkareer.com",
            "Accept": "application/json",
        }

        # Step 1: ì¸íŠ¸ë¡œìŠ¤í™ì…˜ìœ¼ë¡œ ì‹¤ì œ Query í•„ë“œ íŒŒì•… (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        actual_fields = []
        try:
            res = requests.post(
                "https://api.linkareer.com/graphql",
                json={"query": "{ __schema { queryType { fields { name } } } }"},
                headers=gql_headers, timeout=10
            )
            if res.status_code == 200:
                body = res.json()
                if not body.get('errors'):
                    actual_fields = [f['name'] for f in body.get('data',{}).get('__schema',{}).get('queryType',{}).get('fields',[])]
        except Exception as e:
            print(f"  Linkareer ì¸íŠ¸ë¡œìŠ¤í™ì…˜ ì˜ˆì™¸: {e}")

        # Step 2: í•´ì»¤í†¤ê³¼ ë¶€íŠ¸ìº í”„ë¥¼ ëª¨ë‘ ì¡ê¸° ìœ„í•œ ì¿¼ë¦¬ ìƒì„±
        queries = []
        
        # 1. ë¶€íŠ¸ìº í”„/í•´ì»¤í†¤ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¿¼ë¦¬ (í•„ë“œ ì¡´ì¬ ì‹œ)
        if 'activityList' in actual_fields:
            # ë¶€íŠ¸ìº í”„ í•„í„° ì¶”ê°€
            queries.append({"query": '{ activityList(filter: {categoryName: "ë¶€íŠ¸ìº í”„"}, page: 1, pageSize: 20) { list { id title dueDate categories { name } } } }'})
            # í•´ì»¤í†¤ í•„í„° ì¶”ê°€
            queries.append({"query": '{ activityList(filter: {categoryName: "í•´ì»¤í†¤"}, page: 1, pageSize: 20) { list { id title dueDate categories { name } } } }'})
        
        # 2. ë²”ìš© ì¿¼ë¦¬ (í•„í„°ë§ ì—†ì´ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ í›„ í‚¤ì›Œë“œ í•„í„°ë§)
        queries.append({"query": '{ activities(first: 50) { nodes { id title dueDate categories { name } } } }'})

        # ìˆ˜ì§‘ ë° í•„í„°ë§ í‚¤ì›Œë“œ ì •ì˜
        target_keywords = ['í•´ì»¤í†¤', 'hackathon', 'ê³µëª¨ì „', 'ë¶€íŠ¸ìº í”„', 'bootcamp', 'kdt', 'êµìœ¡', 'ì–‘ì„±']
        seen_ids = set()

        for payload in queries:
            try:
                res = requests.post("https://api.linkareer.com/graphql", json=payload, headers=gql_headers, timeout=15)
                if res.status_code != 200: continue
                
                body = res.json()
                if body.get('errors'): continue

                nodes = self._extract_nodes(body.get('data', {}))
                if not nodes: continue

                for node in nodes:
                    nid = node.get('id', '')
                    if not nid or nid in seen_ids: continue
                    
                    title = node.get('title', '')
                    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ë“¤ ì¶”ì¶œ
                    cats_list = [c.get('name', '') for c in (node.get('categories') or [])]
                    cats_str = ' '.join(cats_list).lower()
                    full_text = (title + cats_str).lower()

                    # í‚¤ì›Œë“œ ê²€ì‚¬: íƒ€ê²Ÿ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if any(k in full_text for k in target_keywords):
                        due = (node.get('dueDate') or '')[:10]
                        # ë§ˆê°ì¼ ì§€ë‚œ ê³µê³  ì œì™¸
                        if due and due < today: continue
                        
                        seen_ids.add(nid)
                        
                        # ì•„ì´ì½˜ ë° ì ‘ë‘ì‚¬ ê²°ì • (ë¶€íŠ¸ìº í”„ ìš°ì„ )
                        if any(bk in full_text for bk in ['ë¶€íŠ¸ìº í”„', 'bootcamp', 'kdt', 'êµìœ¡']):
                            prefix = "ğŸ“ [ë¶€íŠ¸ìº í”„]"
                        else:
                            prefix = "ğŸ‡°ğŸ‡· [ë§ì»¤ë¦¬ì–´]"

                        results.append({
                            "title": f"{prefix} {title}",
                            "url": f"https://linkareer.com/activity/{nid}",
                            "host": "Linkareer",
                            "date": due or "ìƒì„¸ í™•ì¸"
                        })
            except Exception as e:
                print(f"  Linkareer GraphQL ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸: {e}")

        return results

    def _extract_nodes(self, data, depth=0):
        """GraphQL ì‘ë‹µì—ì„œ ë…¸ë“œ ë°°ì—´ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰"""
        if depth > 4: return []
        if isinstance(data, list): return data
        if isinstance(data, dict):
            for key in ('nodes', 'list', 'edges', 'items', 'results'):
                if key in data and isinstance(data[key], list):
                    return data[key]
            for v in data.values():
                result = self._extract_nodes(v, depth+1)
                if result: return result
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # run / discord
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self):
        print("ğŸ” í•´ì»¤í†¤ ë° ë¶€íŠ¸ìº í”„ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_hackathons = []
        tasks = [
            ("Devpost",     self.fetch_devpost),
            ("MLH",         self.fetch_mlh),
            ("HackerEarth", self.fetch_hackerearth),
            ("Kaggle",      self.fetch_kaggle),
            ("Hack2Skill",  self.fetch_hack2skill),
            ("DoraHacks",   self.fetch_dorahacks),
            ("Programmers", self.fetch_programmers),    # ì§„í–‰ ëŒ€íšŒ ì—†ìœ¼ë©´ 0ê°œ ì •ìƒ
            ("DevEvent",    self.fetch_devevent),
            ("DACON",       self.fetch_dacon),          # Wevity ëŒ€ì²´
            ("CampusPick",  self.fetch_campuspick),
            ("DACON/AIHub", self.fetch_aihub),          # AIConnect ëŒ€ì²´ (15ê°œ ì„±ê³µ)
            ("Linkareer",   self.fetch_linkareer),
        ]
        for name, fetcher in tasks:
            try:
                found = fetcher()
                print(f"ğŸ“¡ {name}: {len(found)}ê°œ ë°œê²¬")
                all_hackathons.extend(found)
            except Exception as e:
                print(f"âŒ {name} ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±° (title ê¸°ì¤€)
        seen_titles = set()
        deduped = []
        for h in all_hackathons:
            if h['title'] not in seen_titles:
                seen_titles.add(h['title'])
                deduped.append(h)

        new_items = [h for h in deduped if h['title'] not in self.sent_list]
        print(f"ğŸ“Š ìµœì¢… ì‹ ê·œ ê³µê³ : {len(new_items)}ê°œ")
        if not new_items: return
        self.send_to_discord(new_items)
        self.save_sent_list(new_items)

    def send_to_discord(self, hackathons):
        for i in range(0, len(hackathons), 10):
            chunk = hackathons[i:i+10]
            embeds = [{"title": f"ğŸ† {h['title']}", "url": h['url'], "color": 3447003,
                       "fields": [{"name": "í”Œë«í¼", "value": h['host'], "inline": True},
                                  {"name": "ë§ˆê°/ì¼ì •", "value": str(h['date']), "inline": True}]}
                      for h in chunk]
            requests.post(WEBHOOK_URL, json={
                "content": "ğŸš€ **ìƒˆë¡œìš´ ì†Œì‹ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!**" if i == 0 else "",
                "embeds": embeds
            })


if __name__ == "__main__":
    if not WEBHOOK_URL:
        print("âŒ ì˜¤ë¥˜: DISCORD_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        bot = HackathonBot()
        bot.run()
