import os
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time
import re

WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')
DB_FILE = "sent_hackathons.txt"

class HackathonBot:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        self.sent_list = self.load_sent_list()

    def load_sent_list(self):
        if os.path.exists(DB_FILE):
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    def save_sent_list(self, new_items):
        with open(DB_FILE, "a", encoding="utf-8") as f:
            for item in new_items:
                f.write(f"{item['title']}\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆ˜ì§‘ í•¨ìˆ˜ ì„¹ì…˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_devpost(self):
        try:
            h = self.headers.copy()
            h.update({"Accept": "application/json", "Referer": "https://devpost.com/hackathons", "X-Requested-With": "XMLHttpRequest"})
            res = requests.get("https://devpost.com/api/hackathons", params={"status[]": "upcoming", "sort_by": "Recently Added"}, headers=h, timeout=15)
            if res.status_code == 200:
                return [{"title": h['title'], "url": h['url'], "host": "Devpost", "date": h.get('submission_period_dates', 'N/A')}
                        for h in res.json().get('hackathons', [])]
        except: pass
        return []

    def fetch_mlh(self):
        MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}
        try:
            res = requests.get("https://mlh.io/seasons/2026/events", headers=self.headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results, today, seen = [], datetime.now().replace(hour=0,minute=0,second=0,microsecond=0), set()
                for a in soup.find_all('a', href=True):
                    h3 = a.find('h3')
                    if not h3: continue
                    title = h3.get_text(strip=True)
                    if not title or title in seen: continue
                    seen.add(title)
                    link = a['href'].split('?')[0]
                    if not link.startswith('http'): link = "https://mlh.io" + link
                    a_text = a.get_text(separator=' ', strip=True).replace(title, '')
                    date_parts = re.findall(r'([A-Z]{3})\s+(\d{1,2})', a_text)
                    if date_parts:
                        date_str = ' - '.join(f"{m} {d}" for m,d in date_parts) if len(date_parts)>1 else f"{date_parts[0][0]} {date_parts[0][1]}"
                        mon, day = date_parts[-1]
                        end_m = MONTHS.get(mon, 0)
                        if end_m and datetime(today.year, end_m, int(day)) < today: continue
                    else:
                        date_str = "2026 Season"
                    results.append({"title": title, "url": link, "host": "MLH", "date": date_str})
                return results
        except Exception as e:
            print(f"MLH ì˜ˆì™¸: {e}")
        return []

    def fetch_linkareer(self):
        """
        ë§ì»¤ë¦¬ì–´ ìˆ˜ì§‘ ë³µêµ¬ ë²„ì „:
        1. ì„œë²„ê°€ ì°¨ë‹¨í•˜ê¸° ì‰¬ìš´ 'ê²€ìƒ‰(unifiedSearch)' ëŒ€ì‹  'ë¦¬ìŠ¤íŠ¸(activities)' ì¿¼ë¦¬ ì‚¬ìš©
        2. ìˆ˜ì§‘ëœ ë°ì´í„° ë‚´ì—ì„œ 'í•´ì»¤í†¤'ê³¼ 'ë¶€íŠ¸ìº í”„' í‚¤ì›Œë“œë¥¼ ë™ì‹œì— í•„í„°ë§
        3. ì´ì „ì˜ ì•ˆì •ì ì¸ _extract_nodes ì¬ê·€ íƒìƒ‰ í™œìš©
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')
        seen_ids = set()

        gql_headers = {
            "Content-Type": "application/json",
            "User-Agent": self.headers["User-Agent"],
            "Referer": "https://linkareer.com/list/bootcamp", # Refererë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ì •
            "Origin": "https://linkareer.com",
            "Accept": "application/json",
        }

        # ì„œë²„ê°€ ì—ëŸ¬ë¥¼ ë±‰ì§€ ì•ŠëŠ” ê°€ì¥ ëŒ€ì¤‘ì ì¸(Public) ì¿¼ë¦¬ íŒ¨í„´ 2ê°€ì§€
        queries = [
            # íŒ¨í„´ A: ìµœì‹  í™œë™ ì „ì²´ ë…¸ë“œ (ë¶€íŠ¸ìº í”„, ê³µëª¨ì „ ì„ì—¬ ìˆìŒ)
            {"query": '{ activities(first: 50) { nodes { id title dueDate hostName categories { name } } } }'},
            # íŒ¨í„´ B: í™œë™ ë¦¬ìŠ¤íŠ¸ (ë³´ì¡°ìš©)
            {"query": '{ activityList(page: 1, pageSize: 50) { list { id title dueDate hostName categories { name } } } }'}
        ]

        # í•„í„°ë§í•  í‚¤ì›Œë“œ í†µí•© (í•´ì»¤í†¤ + ë¶€íŠ¸ìº í”„)
        target_keywords = ['í•´ì»¤í†¤', 'hackathon', 'ê³µëª¨ì „', 'ë¶€íŠ¸ìº í”„', 'bootcamp', 'kdt', 'êµìœ¡', 'ì–‘ì„±']

        for payload in queries:
            try:
                res = requests.post("https://api.linkareer.com/graphql", json=payload, headers=gql_headers, timeout=15)
                
                # ë§Œì•½ 400 ì—ëŸ¬ ë“±ì´ ë‚˜ë©´ ë‹¤ìŒ íŒ¨í„´ìœ¼ë¡œ ì´ë™
                if res.status_code != 200:
                    continue
                
                body = res.json()
                if body.get('errors'):
                    continue

                # ì§ˆë¬¸ìë‹˜ì´ ì´ì „ì— ì„±ê³µí–ˆë˜ 'ì¬ê·€ íƒìƒ‰' í•¨ìˆ˜ í˜¸ì¶œ
                nodes = self._extract_nodes(body.get('data', {}))
                if not nodes:
                    continue

                for node in nodes:
                    nid = node.get('id')
                    if not nid or nid in seen_ids:
                        continue
                    
                    title = node.get('title', '')
                    # ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì´ë¦„ë“¤ì„ ê³µë°±ìœ¼ë¡œ í•©ì¹¨
                    cats = ' '.join(c.get('name','') for c in (node.get('categories') or []))
                    
                    # ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ì „ì²´ì—ì„œ í‚¤ì›Œë“œ ê²€ì‚¬
                    full_text = (title + " " + cats).lower()

                    if any(k in full_text for k in target_keywords):
                        due = (node.get('dueDate') or '')[:10]
                        
                        # ë§ˆê°ê¸°í•œ ì²´í¬
                        if due and due < today:
                            continue

                        seen_ids.add(nid)
                        
                        # ë¶€íŠ¸ìº í”„ ì„±ê²©ì¸ì§€ í™•ì¸í•˜ì—¬ ì•„ì´ì½˜ ë¶„ê¸°
                        is_bootcamp = any(k in full_text for k in ['ë¶€íŠ¸ìº í”„', 'bootcamp', 'kdt', 'êµìœ¡'])
                        icon = "ğŸ“ [ë¶€íŠ¸ìº í”„]" if is_bootcamp else "ğŸ‡°ğŸ‡· [ë§ì»¤ë¦¬ì–´]"
                        
                        results.append({
                            "title": f"{icon} {title}",
                            "url": f"https://linkareer.com/activity/{nid}",
                            "host": node.get('hostName') or "Linkareer",
                            "date": due or "ìƒì„¸ í™•ì¸"
                        })
            except Exception as e:
                print(f"  Linkareer ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

        # ëª¨ë“  ì¿¼ë¦¬ ì‹œë„ í›„ ê²°ê³¼ ë°˜í™˜
        return results

    def fetch_campuspick(self):
        try:
            h = self.headers.copy()
            h.update({"Content-Type": "application/x-www-form-urlencoded", "Origin": "https://www2.campuspick.com", "Referer": "https://www2.campuspick.com/"})
            today, results = datetime.now().strftime('%Y-%m-%d'), []
            for cat_id in [108, 111]: # 108: ê³µëª¨ì „, 111: êµìœ¡/ê°•ì—°
                res = requests.post("https://api2.campuspick.com/find/activity/list", data={"target":1,"limit":20,"offset":0,"categoryId":cat_id}, headers=h, timeout=15)
                if res.status_code == 200:
                    activities = res.json().get("result", {}).get("activities", [])
                    for a in activities:
                        if a.get("endDate","") >= today:
                            prefix = "ğŸ“ [ë¶€íŠ¸ìº í”„/êµìœ¡]" if cat_id == 111 else "ğŸ‡°ğŸ‡· [ìº í¼ìŠ¤í”½]"
                            results.append({"title": f"{prefix} {a['title']}", "url": f"https://www2.campuspick.com/contest/view?id={a['id']}", "host": "CampusPick", "date": a.get("endDate","ìƒì„¸ í™•ì¸")})
            return results
        except Exception as e: print(f"CampusPick ì˜ˆì™¸: {e}")
        return []

    def fetch_devevent(self):
        try:
            now = datetime.now()
            url = f"https://raw.githubusercontent.com/brave-people/Dev-Event/master/end_event/{now.year}/{str(now.year)[2:]}_{str(now.month).zfill(2)}.md"
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                results = []
                for m in re.finditer(r'__\[([^\]]+)\]\((https?://[^\)]+)\)__', res.text):
                    title, link = m.group(1), m.group(2)
                    target_keywords = ['í•´ì»¤í†¤', 'hackathon', 'ê³µëª¨ì „', 'ê²½ì§„ëŒ€íšŒ', 'ë¶€íŠ¸ìº í”„', 'bootcamp', 'êµìœ¡', 'kdt', 'ì–‘ì„±']
                    if any(k in title.lower() for k in target_keywords):
                        icon = "ğŸ“" if any(b in title.lower() for b in ['ë¶€íŠ¸ìº í”„', 'êµìœ¡', 'kdt']) else "ğŸ‡°ğŸ‡·"
                        results.append({"title": f"{icon} [ë°ë¸Œì´ë²¤íŠ¸] {title}", "url": link, "host": "DevEvent", "date": "ìƒì„¸ í™•ì¸"})
                return results
        except: pass
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìœ í‹¸ë¦¬í‹° ë° ì‹¤í–‰ ì„¹ì…˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _extract_nodes(self, data, depth=0):
        if depth > 4: return []
        if isinstance(data, list): return data
        if isinstance(data, dict):
            for key in ('nodes', 'list', 'edges', 'items'):
                if key in data and isinstance(data[key], list): return data[key]
            for v in data.values():
                res = self._extract_nodes(v, depth+1)
                if res: return res
        return []

    def run(self):
        print("ğŸ” í•´ì»¤í†¤ ë° ë¶€íŠ¸ìº í”„ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_items = []
        tasks = [
            ("Devpost", self.fetch_devpost), ("MLH", self.fetch_mlh),
            ("DevEvent", self.fetch_devevent), ("CampusPick", self.fetch_campuspick),
            ("Linkareer", self.fetch_linkareer)
        ]
        
        for name, fetcher in tasks:
            try:
                found = fetcher()
                print(f"ğŸ“¡ {name}: {len(found)}ê°œ ë°œê²¬")
                all_items.extend(found)
            except Exception as e: print(f"âŒ {name} ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€) ë° ì‹ ê·œ í•­ëª© í•„í„°ë§
        seen_titles, deduped = set(), []
        for item in all_items:
            if item['title'] not in seen_titles:
                seen_titles.add(item['title'])
                deduped.append(item)

        new_items = [i for i in deduped if i['title'] not in self.sent_list]
        print(f"ğŸ“Š ìµœì¢… ì‹ ê·œ ê³µê³ : {len(new_items)}ê°œ")
        
        if new_items:
            self.send_to_discord(new_items)
            self.save_sent_list(new_items)

    def send_to_discord(self, items):
        for i in range(0, len(items), 10):
            chunk = items[i:i+10]
            embeds = [{"title": f"âœ¨ {h['title']}", "url": h['url'], "color": 5814783,
                       "fields": [{"name": "í”Œë«í¼", "value": h['host'], "inline": True},
                                  {"name": "ë§ˆê°/ì¼ì •", "value": str(h['date']), "inline": True}]}
                      for h in chunk]
            requests.post(WEBHOOK_URL, json={
                "content": "ğŸš€ **ìƒˆë¡œìš´ ì†Œì‹ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤!**" if i == 0 else "",
                "embeds": embeds
            })

if __name__ == "__main__":
    if WEBHOOK_URL:
        HackathonBot().run()
    else:
        print("âŒ DISCORD_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
