import os
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time
import re

# ì„¤ì •
WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')
DB_FILE = "sent_hackathons.txt"

class HackathonBot:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        self.base_headers = self.headers.copy()
        self.sent_list = self.load_sent_list()

    def load_sent_list(self):
        if os.path.exists(DB_FILE):
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    def save_sent_list(self, new_items):
        with open(DB_FILE, "a", encoding="utf-8") as f:
            for item in new_items:
                f.write(f"{item['title']}\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê¸°ì¡´ ì •ìƒ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_devpost(self):
        try:
            custom_headers = self.headers.copy()
            custom_headers.update({
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Referer": "https://devpost.com/hackathons",
                "X-Requested-With": "XMLHttpRequest"
            })
            params = {"status[]": "upcoming", "sort_by": "Recently Added"}
            url = "https://devpost.com/api/hackathons"
            res = requests.get(url, params=params, headers=custom_headers, timeout=15)
            if res.status_code == 200:
                data = res.json()
                hackathons = data.get('hackathons', [])
                return [{"title": h['title'], "url": h['url'], "host": "Devpost", "date": h.get('submission_period_dates', 'N/A')} for h in hackathons]
            return []
        except: return []

    def fetch_mlh(self):
        MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,
                  'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}
        try:
            url = "https://mlh.io/seasons/2026/events"
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results = []
                today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                seen = set()
                for a in soup.find_all('a', href=True):
                    h3 = a.find('h3')
                    if not h3:
                        continue
                    title = h3.get_text(strip=True)
                    if not title or title in seen:
                        continue
                    seen.add(title)
                    link = a['href'].split('?')[0]
                    if not link.startswith('http'):
                        link = "https://mlh.io" + link
                    a_text = a.get_text(separator=' ', strip=True).replace(title, '')
                    date_parts = re.findall(r'([A-Z]{3})\s+(\d{1,2})', a_text)
                    if date_parts:
                        date_str = ' - '.join(f"{m} {d}" for m, d in date_parts) if len(date_parts) > 1 else f"{date_parts[0][0]} {date_parts[0][1]}"
                        mon, day = date_parts[-1]
                        end_m = MONTHS.get(mon, 0)
                        if end_m:
                            event_end = datetime(today.year, end_m, int(day))
                            if event_end < today:
                                continue
                    else:
                        date_str = "2026 Season"
                    results.append({"title": title, "url": link, "host": "MLH", "date": date_str})
                print(f"ğŸ“¡ MLH: {len(results)}ê°œ ì¶”ì¶œ ì„±ê³µ (ì¢…ë£Œ ì´ë²¤íŠ¸ ì œì™¸)")
                return results
        except Exception as e:
            print(f"MLH í¬ë¡¤ë§ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

    def fetch_kaggle(self):
        username = os.environ.get('KAGGLE_USERNAME', '')
        key = os.environ.get('KAGGLE_KEY', '')
        print(f"DEBUG: Username length: {len(username)}")
        print(f"DEBUG: Key length: {len(key)}")
        if not username or not key:
            print("âŒ ì˜¤ë¥˜: Kaggle í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            res = requests.get(
                'https://www.kaggle.com/api/v1/competitions/list',
                params={'sortBy': 'latestDeadline', 'pageSize': 20},
                auth=(username, key),
                headers=self.headers, timeout=15
            )
            if res.status_code != 200:
                print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ (Status: {res.status_code}): {res.text}")
                return []
            data = res.json()
            results = []
            for c in data:
                title = c.get('title', '')
                deadline = (c.get('deadline') or '')[:10]
                if not title or (deadline and deadline < today):
                    continue
                ref = c.get('ref') or c.get('id', '')
                results.append({
                    "title": title,
                    "url": f"https://www.kaggle.com/competitions/{ref}",
                    "host": "Kaggle",
                    "date": deadline or "ìƒì„¸ í™•ì¸"
                })
            print(f"âœ… {len(results)}ê°œì˜ í™œì„± ê²½ì§„ëŒ€íšŒë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results
        except Exception as e:
            print(f"âŒ Kaggle í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return []

    def fetch_hack2skill(self):
        try:
            res = requests.get('https://hack2skill.com/', headers=self.headers, timeout=15)
            if res.status_code != 200:
                return []
            soup = BeautifulSoup(res.text, 'html.parser')
            flagship = soup.find(class_='flagshipEventsSlider')
            if not flagship:
                return []
            today = datetime.now()
            results = []
            seen = set()
            for a in flagship.find_all('a', href=re.compile(r'hack2skill\.com')):
                url = a['href'].split('?')[0]
                if url in seen:
                    continue
                card = a.find_parent('div', class_=re.compile(r'w-\[16rem\]'))
                if not card:
                    continue
                h5s = card.find_all('h5')
                if not h5s:
                    continue
                title = h5s[0].get_text(strip=True)
                if not title:
                    continue
                date_str = h5s[-1].get_text(strip=True) if len(h5s) > 1 else ''
                try:
                    event_date = datetime.strptime(date_str, '%a %b %d %Y')
                    if event_date < today:
                        continue
                except ValueError:
                    pass
                seen.add(url)
                results.append({"title": title, "url": url, "host": "Hack2Skill", "date": date_str})
            return results
        except Exception as e:
            print(f"Hack2Skill í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_dorahacks(self):
        try:
            import time as _time
            res = requests.get(
                "https://dorahacks.io/api/hackathon",
                params={"status": "open", "limit": 20},
                headers=self.headers, timeout=15
            )
            if res.status_code == 200:
                now_ts = _time.time()
                results = []
                for h in res.json().get('results', []):
                    title = h.get('title', '')
                    if not title:
                        continue
                    end_ts = h.get('end_time')
                    if end_ts and int(end_ts) < now_ts:
                        continue
                    results.append({
                        "title": title,
                        "url": f"https://dorahacks.io/hackathon/{h.get('id', '')}",
                        "host": "DoraHacks",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
                return results
        except Exception as e:
            print(f"DoraHacks í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_devevent(self):
        try:
            now = datetime.now()
            year_short = str(now.year)[2:]
            month = str(now.month).zfill(2)
            url = f"https://raw.githubusercontent.com/brave-people/Dev-Event/master/end_event/{now.year}/{year_short}_{month}.md"
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                results = []
                for m in re.finditer(r'__\[([^\]]+)\]\((https?://[^\)]+)\)__', res.text):
                    title, link = m.group(1), m.group(2)
                    if any(k in title for k in ['í•´ì»¤í†¤', 'Hackathon', 'hackathon', 'ê³µëª¨ì „', 'ê²½ì§„ëŒ€íšŒ']):
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [ë°ë¸Œì´ë²¤íŠ¸] {title}",
                            "url": link,
                            "host": "DevEvent",
                            "date": "ìƒì„¸ í™•ì¸"
                        })
                return results
        except Exception as e:
            print(f"DevEvent í¬ë¡¤ë§ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

    def fetch_campuspick(self):
        try:
            api_headers = self.headers.copy()
            api_headers.update({
                "Content-Type": "application/x-www-form-urlencoded",
                "Origin": "https://www2.campuspick.com",
                "Referer": "https://www2.campuspick.com/contest?category=108",
            })
            today = datetime.now().strftime('%Y-%m-%d')
            results = []
            for offset in range(0, 40, 20):
                res = requests.post(
                    "https://api2.campuspick.com/find/activity/list",
                    data={"target": 1, "limit": 20, "offset": offset, "categoryId": 108},
                    headers=api_headers, timeout=15
                )
                if res.status_code != 200:
                    break
                activities = res.json().get("result", {}).get("activities", [])
                if not activities:
                    break
                valid = [a for a in activities if a.get("endDate", "") >= today]
                for a in valid:
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [ìº í¼ìŠ¤í”½] {a['title']}",
                        "url": f"https://www2.campuspick.com/contest/view?id={a['id']}",
                        "host": "CampusPick",
                        "date": a.get("endDate", "ìƒì„¸ í™•ì¸")
                    })
                if not valid:
                    break
            return results
        except Exception as e:
            print(f"CampusPick í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆ˜ì •ëœ í•¨ìˆ˜ë“¤
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_devfolio(self):
        """
        [ìˆ˜ì • ì›ì¸] __NEXT_DATA__ JSON êµ¬ì¡° ë³€ê²½ìœ¼ë¡œ open_hackathons í‚¤ ì†Œì‹¤
        [í•´ê²°]  HTMLì—ì„œ *.devfolio.co ì„œë¸Œë„ë©”ì¸ <a> íƒœê·¸ ì§ì ‘ íŒŒì‹±
                ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸:
                  <a href="https://campfire-hackathon.devfolio.co/">
                    <h3>Campfire Hackathon</h3>
                  </a>
        """
        headers = self.headers.copy()
        headers.update({
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://devfolio.co/",
        })
        results = []
        seen = set()

        try:
            res = requests.get("https://devfolio.co/hackathons", headers=headers, timeout=15)
            if res.status_code != 200:
                print(f"  Devfolio ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}")
                return []

            soup = BeautifulSoup(res.text, 'html.parser')

            for a in soup.find_all('a', href=True):
                href = a['href'].rstrip('/')

                # https://<slug>.devfolio.co í˜•íƒœë§Œ ìˆ˜ì§‘
                m = re.match(r'https://([^./]+)\.devfolio\.co$', href)
                if not m:
                    continue
                subdomain = m.group(1)
                if subdomain in ('www', 'assets'):
                    continue
                if href in seen:
                    continue
                seen.add(href)

                # ì œëª©: <h3> ìš°ì„ 
                h_tag = a.find('h3') or a.find('h2') or a.find('h4')
                title = h_tag.get_text(strip=True) if h_tag else a.get_text(strip=True)
                if not title:
                    continue

                results.append({
                    "title": title,
                    "url": href,
                    "host": "Devfolio",
                    "date": "ìƒì„¸ í™•ì¸"
                })

        except Exception as e:
            print(f"Devfolio í¬ë¡¤ë§ ì˜ˆì™¸: {e}")

        return results

    def fetch_programmers(self):
        """
        [ìˆ˜ì • ì›ì¸] programmers.co.kr â†’ career.programmers.co.kr ë„ë©”ì¸ ì´ì „
                   /api/competitions ì—”ë“œí¬ì¸íŠ¸ê°€ êµ¬ ë„ë©”ì¸ì—ì„œ 404
        [í•´ê²°]  1ì°¨: career.programmers.co.kr/api/competitions (JSON)
                2ì°¨: career.programmers.co.kr/competitions (HTML íŒŒì‹±)
        """
        today = datetime.now().strftime('%Y-%m-%d')
        results = []

        # 1ì°¨: career API JSON
        try:
            res = requests.get(
                "https://career.programmers.co.kr/api/competitions",
                headers=self.headers, timeout=15
            )
            if res.status_code == 200:
                data = res.json()
                items = data if isinstance(data, list) else data.get('competitions', [])
                for c in items:
                    if c.get('statusLabel') == 'ended':
                        continue
                    end_at = c.get('receiptEndAt') or c.get('endAt') or ''
                    if end_at and end_at[:10] < today:
                        continue
                    title = c.get('title', '')
                    href  = c.get('href', '') or c.get('url', '')
                    if not title:
                        continue
                    full_url = (
                        f"https://career.programmers.co.kr{href}"
                        if href.startswith('/') else href
                    )
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [í”„ë¡œê·¸ë˜ë¨¸ìŠ¤] {title}",
                        "url": full_url,
                        "host": "Programmers",
                        "date": end_at[:10] if end_at else "ìƒì„¸ í™•ì¸"
                    })
                if results:
                    return results
        except Exception as e:
            print(f"  Programmers career API ì˜ˆì™¸: {e}")

        # 2ì°¨: HTML íŒŒì‹±
        try:
            res = requests.get(
                "https://career.programmers.co.kr/competitions",
                headers=self.headers, timeout=15
            )
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                seen = set()
                for a in soup.find_all('a', href=re.compile(r'/competitions/\d+')):
                    href = a['href']
                    path_key = re.match(r'/competitions/\d+', href)
                    if not path_key or path_key.group() in seen:
                        continue
                    seen.add(path_key.group())

                    parent = a.find_parent(['li', 'article', 'div'])
                    if parent and any(k in parent.get_text() for k in ['ì ‘ìˆ˜ë§ˆê°', 'ì¢…ë£Œ']):
                        continue

                    h_tag = a.find(['h3', 'h2', 'h4', 'strong', 'p'])
                    title = h_tag.get_text(strip=True) if h_tag else a.get_text(strip=True)
                    if not title:
                        continue

                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [í”„ë¡œê·¸ë˜ë¨¸ìŠ¤] {title}",
                        "url": f"https://career.programmers.co.kr{href}",
                        "host": "Programmers",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
            else:
                print(f"  Programmers HTML ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}")
        except Exception as e:
            print(f"  Programmers HTML íŒŒì‹± ì˜ˆì™¸: {e}")

        return results

    def fetch_wevity(self):
        """
        [ìˆ˜ì • ì›ì¸] GitHub Actions IP(ë°ì´í„°ì„¼í„°)ë¥¼ Cloudflare WAFê°€ êµ¬ì¡°ì ìœ¼ë¡œ 403 ì°¨ë‹¨.
                   ë‹¨ìˆœ í—¤ë” ê°•í™”ë¡œëŠ” TLS fingerprint ì°¨ì´ë¡œ ì¸í•´ ìš°íšŒ ë¶ˆê°€.
        [í•´ê²°]  1ì°¨: Wevity ê°•í™” í—¤ë”ë¡œ ì¬ì‹œë„
                2ì°¨: ì°¨ë‹¨ ì‹œ ê³µëª¨ì „365(contestkorea.com)ë¡œ ëŒ€ì²´ ìˆ˜ì§‘
                     - IT/SW ê³µëª¨ì „ ì¹´í…Œê³ ë¦¬ (wevity IT/SWì™€ ë™ì¼ ë°ì´í„° í¬í•¨)
        """
        results = []

        # 1ì°¨: Wevity ê°•í™” í—¤ë”
        try:
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
                'DNT': '1',
            })
            main_res = session.get('https://www.wevity.com/', timeout=15)
            time.sleep(2)

            if main_res.status_code == 200:
                session.headers.update({
                    'Referer': 'https://www.wevity.com/',
                    'Sec-Fetch-Site': 'same-origin',
                })
                for cidx, cat_label in [('20', 'ê¸°íš'), ('21', 'IT/SW')]:
                    url = f'https://www.wevity.com/?c=find&s=1&gub=1&cidx={cidx}'
                    res = session.get(url, timeout=15)
                    if res.status_code != 200:
                        print(f"  Wevity {cat_label} HTTP {res.status_code}")
                        continue
                    soup = BeautifulSoup(res.text, 'html.parser')
                    ul = soup.find('ul', class_='list')
                    if not ul:
                        continue
                    for li in ul.find_all('li'):
                        if 'top' in li.get('class', []):
                            continue
                        dday_span = li.find('span', class_='dday')
                        if dday_span and dday_span.get_text(strip=True) == 'ë§ˆê°':
                            continue
                        tit_div = li.find('div', class_='tit')
                        if not tit_div:
                            continue
                        a_tag = tit_div.find('a', href=True)
                        if not a_tag:
                            continue
                        title = a_tag.get_text(strip=True)
                        href = a_tag['href']
                        full_url = "https://www.wevity.com/" + href if href.startswith('?') else href
                        day_div = li.find('div', class_='day')
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [ìœ„ë¹„í‹°-{cat_label}] {title}",
                            "url": full_url,
                            "host": "Wevity",
                            "date": day_div.get_text(strip=True) if day_div else "ìƒì„¸ í™•ì¸"
                        })
                    time.sleep(1.5)

                if results:
                    return results

        except Exception as e:
            print(f"  Wevity ì§ì ‘ ì ‘ê·¼ ì˜ˆì™¸: {e}")

        # 2ì°¨: ê³µëª¨ì „365 ëŒ€ì²´ (IT/SW, ê²Œì„/ì†Œí”„íŠ¸ì›¨ì–´ ì¹´í…Œê³ ë¦¬)
        print("  Wevity ì°¨ë‹¨ â†’ ê³µëª¨ì „365 ëŒ€ì²´ ìˆ˜ì§‘")
        try:
            contest_headers = self.headers.copy()
            contest_headers.update({
                'Referer': 'https://www.contestkorea.com/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9',
            })
            res = requests.get(
                "https://www.contestkorea.com/sub/list.php",
                params={"Txt_bcode": "030504001", "Txt_sele": "ing"},
                headers=contest_headers, timeout=15
            )
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                seen = set()
                for a in soup.find_all('a', href=re.compile(r'int_No=\d+')):
                    href = a['href']
                    m = re.search(r'int_No=(\d+)', href)
                    if not m or m.group(1) in seen:
                        continue
                    seen.add(m.group(1))
                    title = a.get_text(strip=True)
                    if not title or len(title) < 4:
                        continue
                    full_url = (
                        f"https://www.contestkorea.com{href}"
                        if href.startswith('/') else href
                    )
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [ê³µëª¨ì „365] {title}",
                        "url": full_url,
                        "host": "ContestKorea",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
        except Exception as e:
            print(f"  ê³µëª¨ì „365 ëŒ€ì²´ ìˆ˜ì§‘ ì˜ˆì™¸: {e}")

        return results

    def fetch_aiconnect(self):
        """
        [ìˆ˜ì • ì›ì¸] aiconnect.krì€ ì™„ì „í•œ CSR(í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë Œë”ë§).
                   ì„œë²„ì—ì„œ ë°˜í™˜í•˜ëŠ” HTMLì—ëŠ” ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ HTML íŒŒì‹± ë¶ˆê°€.
                   window.__NUXT__ ë˜í•œ ë°ì´í„°ê°€ ì—†ëŠ” ë¹ˆ ìƒíƒœ.
        [í•´ê²°]  ë‚´ë¶€ REST API íŒ¨í„´ ìˆœì°¨ ì‹œë„ (ë¸Œë¼ìš°ì € Network íƒ­ ê¸°ì¤€ ì¶”ì •)
                ì‹¤íŒ¨ ì‹œ AI Hub(aihub.or.kr) ì±Œë¦°ì§€ ëª©ë¡ìœ¼ë¡œ ëŒ€ì²´
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')

        api_headers = self.headers.copy()
        api_headers.update({
            "Accept": "application/json, text/plain, */*",
            "Referer": "https://aiconnect.kr/competition/list",
            "Origin": "https://aiconnect.kr",
        })

        # aiconnect.kr ë‚´ë¶€ API í›„ë³´ (URL íŒ¨í„´ /main/competition/detail/{id} ì—ì„œ ì—­ì¶”ë¡ )
        api_candidates = [
            ("GET",  "https://aiconnect.kr/api/v2/competition/list",    {"status": "open"}),
            ("GET",  "https://aiconnect.kr/api/v1/competition/list",    {"page": 1, "limit": 20}),
            ("GET",  "https://aiconnect.kr/api/competitions",            {"status": "open"}),
            ("GET",  "https://aiconnect.kr/main/api/competition/list",  {"page": 1}),
            ("POST", "https://aiconnect.kr/api/competition/list",        {}),
        ]

        for method, url, params in api_candidates:
            try:
                if method == "GET":
                    res = requests.get(url, params=params, headers=api_headers, timeout=10)
                else:
                    res = requests.post(url, json=params, headers=api_headers, timeout=10)

                if res.status_code != 200:
                    continue

                data = res.json()
                items = (
                    data if isinstance(data, list)
                    else data.get('data', data.get('competitions',
                         data.get('list', data.get('result', []))))
                )
                if not isinstance(items, list) or not items:
                    continue

                for c in items:
                    title = c.get('title') or c.get('name', '')
                    cid   = c.get('id') or c.get('competitionId') or c.get('seq', '')
                    end_d = (c.get('endDate') or c.get('end_date') or '')[:10]
                    if end_d and end_d < today:
                        continue
                    if title:
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [AI Connect] {title}",
                            "url": f"https://aiconnect.kr/main/competition/detail/{cid}/competitionInfo",
                            "host": "AIConnect",
                            "date": end_d or "ìƒì„¸ í™•ì¸"
                        })
                if results:
                    print(f"  AIConnect API ì„±ê³µ: {url}")
                    return results

            except Exception:
                continue

        # ëŒ€ì²´: AI Hub ì±Œë¦°ì§€ ëª©ë¡
        print("  AIConnect API ëª¨ë‘ ì‹¤íŒ¨ â†’ AI Hub ëŒ€ì²´ ìˆ˜ì§‘")
        try:
            aihub_headers = self.headers.copy()
            aihub_headers.update({
                "Accept": "application/json",
                "Referer": "https://aihub.or.kr/",
            })
            # AI Hub ê³µê°œ ì±Œë¦°ì§€ API
            res = requests.get(
                "https://aihub.or.kr/api/v1/board/challenge/list",
                params={"pageIndex": 1, "pageSize": 20, "searchStatus": "ING"},
                headers=aihub_headers, timeout=15
            )
            if res.status_code == 200:
                data = res.json()
                items = data.get('data', data.get('list', []))
                for c in (items if isinstance(items, list) else []):
                    title = c.get('title') or c.get('challengeTitle', '')
                    cid   = c.get('challengeId') or c.get('id', '')
                    end_d = (c.get('endDate') or '')[:10]
                    if not title:
                        continue
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [AI Hub] {title}",
                        "url": f"https://aihub.or.kr/challenge/detail?challengeId={cid}",
                        "host": "AIHub",
                        "date": end_d or "ìƒì„¸ í™•ì¸"
                    })
        except Exception as e:
            print(f"  AI Hub ëŒ€ì²´ ìˆ˜ì§‘ ì˜ˆì™¸: {e}")

        return results

    def fetch_linkareer(self):
        """
        [ìˆ˜ì • ì›ì¸] GraphQL ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜.
                   - ê¸°ì¡´ ì¿¼ë¦¬ { activities { nodes { id title ... } } } ëŠ”
                     ì‹¤ì œ ìŠ¤í‚¤ë§ˆì™€ ë‹¬ë¼ errors ë˜ëŠ” ë¹ˆ nodes ë°˜í™˜.
                   - í•´ì»¤í†¤ í•„í„° ì—†ì´ ì „ì²´ ì¡°íšŒ ì‹œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ í•­ëª©ì´ í¬í•¨ ì•ˆ ë¨.
        [í•´ê²°]  ì—¬ëŸ¬ GraphQL ì¿¼ë¦¬ íŒ¨í„´ì„ ìˆœì°¨ ì‹œë„ (errors ìˆìœ¼ë©´ ë‹¤ìŒìœ¼ë¡œ).
                ì „ì²´ ì¡°íšŒ í›„ í´ë¼ì´ì–¸íŠ¸ í•„í„°ë§ìœ¼ë¡œ ìµœí›„ fallback.
                ëª¨ë‘ ì‹¤íŒ¨ ì‹œ REST /v1/activities ì‹œë„.
        """
        results = []
        today = datetime.now().strftime('%Y-%m-%d')

        gql_headers = {
            "Content-Type": "application/json",
            "User-Agent": self.headers["User-Agent"],
            "Referer": "https://linkareer.com/",
            "Origin": "https://linkareer.com",
        }

        # ìŠ¤í‚¤ë§ˆ ë¶ˆí™•ì‹¤ì„± ëŒ€ë¹„ ì—¬ëŸ¬ ì¿¼ë¦¬ íŒ¨í„´ ìˆœì°¨ ì‹œë„
        queries = [
            # íŒ¨í„´ A: categoryName_contains í•„í„°
            {"query": """
                query {
                  activityList(
                    filter: { categoryName_contains: "í•´ì»¤í†¤" }
                    pagination: { page: 1, pageSize: 20 }
                  ) {
                    activities { id title dueDate categories { name } }
                  }
                }
            """},
            # íŒ¨í„´ B: relay-style + type í•„í„°
            {"query": """
                query {
                  activities(first: 20, filter: { type: HACKATHON }) {
                    nodes { id title dueDate }
                  }
                }
            """},
            # íŒ¨í„´ C: keyword íŒŒë¼ë¯¸í„°
            {"query": """
                query {
                  activities(first: 20, keyword: "í•´ì»¤í†¤") {
                    nodes { id title dueDate }
                  }
                }
            """},
            # íŒ¨í„´ D: ì „ì²´ ì¡°íšŒ í›„ í´ë¼ì´ì–¸íŠ¸ í•„í„° (ìµœí›„ ìˆ˜ë‹¨)
            {"query": """
                {
                  activities(first: 50) {
                    nodes { id title dueDate categories { name } }
                  }
                }
            """},
        ]

        for payload in queries:
            try:
                res = requests.post(
                    "https://api.linkareer.com/graphql",
                    json=payload,
                    headers=gql_headers,
                    timeout=15
                )
                if res.status_code != 200:
                    continue

                body = res.json()
                if body.get('errors'):
                    continue  # ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ â†’ ë‹¤ìŒ íŒ¨í„´

                data = body.get('data', {})
                nodes = []
                for key in data:
                    val = data[key]
                    if isinstance(val, dict):
                        nodes = val.get('nodes', val.get('activities', []))
                    elif isinstance(val, list):
                        nodes = val
                    if nodes:
                        break

                if not nodes:
                    continue

                for node in nodes:
                    title = node.get('title', '')
                    cats  = ' '.join(c.get('name', '') for c in (node.get('categories') or []))
                    if not any(k in title + cats for k in ['í•´ì»¤í†¤', 'Hackathon', 'hackathon', 'ê³µëª¨ì „']):
                        continue
                    nid = node.get('id', '')
                    due = (node.get('dueDate') or '')[:10]
                    if due and due < today:
                        continue
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [ë§ì»¤ë¦¬ì–´] {title}",
                        "url": f"https://linkareer.com/activity/{nid}",
                        "host": "Linkareer",
                        "date": due or "ìƒì„¸ í™•ì¸"
                    })

                if results:
                    return results

            except Exception as e:
                print(f"  Linkareer GraphQL íŒ¨í„´ ì˜ˆì™¸: {e}")

        # 2ì°¨: REST API
        try:
            for endpoint in [
                "https://api.linkareer.com/v1/activities",
                "https://linkareer.com/api/v1/activities",
            ]:
                res = requests.get(
                    endpoint,
                    params={"category": "í•´ì»¤í†¤", "status": "open", "limit": 20},
                    headers={"Accept": "application/json", "User-Agent": self.headers["User-Agent"]},
                    timeout=15
                )
                if res.status_code == 200:
                    data = res.json()
                    items = data if isinstance(data, list) else data.get('activities', data.get('list', []))
                    for item in (items if isinstance(items, list) else []):
                        title = item.get('title', '')
                        nid   = item.get('id', '')
                        due   = (item.get('dueDate') or item.get('due_date') or '')[:10]
                        if due and due < today:
                            continue
                        if title:
                            results.append({
                                "title": f"ğŸ‡°ğŸ‡· [ë§ì»¤ë¦¬ì–´] {title}",
                                "url": f"https://linkareer.com/activity/{nid}",
                                "host": "Linkareer",
                                "date": due or "ìƒì„¸ í™•ì¸"
                            })
                    if results:
                        return results
        except Exception as e:
            print(f"  Linkareer REST ì˜ˆì™¸: {e}")

        return results

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # run / discord
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self):
        print("ğŸ” í•´ì»¤í†¤ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_hackathons = []

        tasks = [
            ("Devpost",     self.fetch_devpost),
            ("MLH",         self.fetch_mlh),
            ("Devfolio",    self.fetch_devfolio),
            ("Kaggle",      self.fetch_kaggle),
            ("Hack2Skill",  self.fetch_hack2skill),
            ("DoraHacks",   self.fetch_dorahacks),
            ("Programmers", self.fetch_programmers),
            ("DevEvent",    self.fetch_devevent),
            ("Wevity",      self.fetch_wevity),
            ("CampusPick",  self.fetch_campuspick),
            ("AIConnect",   self.fetch_aiconnect),
            ("Linkareer",   self.fetch_linkareer),
        ]

        for name, fetcher in tasks:
            try:
                found = fetcher()
                print(f"ğŸ“¡ {name}: {len(found)}ê°œ ë°œê²¬")
                all_hackathons.extend(found)
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

        new_items = [h for h in all_hackathons if h['title'] not in self.sent_list]
        print(f"ğŸ“Š ìµœì¢… ì‹ ê·œ ê³µê³ : {len(new_items)}ê°œ")

        if not new_items:
            return

        self.send_to_discord(new_items)
        self.save_sent_list(new_items)

    def send_to_discord(self, hackathons):
        for i in range(0, len(hackathons), 10):
            chunk = hackathons[i:i+10]
            embeds = []
            for h in chunk:
                embeds.append({
                    "title": f"ğŸ† {h['title']}",
                    "url": h['url'],
                    "color": 3447003,
                    "fields": [
                        {"name": "í”Œë«í¼", "value": h['host'], "inline": True},
                        {"name": "ë§ˆê°/ì¼ì •", "value": str(h['date']), "inline": True}
                    ]
                })
            payload = {
                "content": "ğŸš€ **ìƒˆë¡œìš´ í•´ì»¤í†¤ ëŒ€íšŒê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!**" if i == 0 else "",
                "embeds": embeds
            }
            requests.post(WEBHOOK_URL, json=payload)


if __name__ == "__main__":
    if not WEBHOOK_URL:
        print("âŒ ì˜¤ë¥˜: DISCORD_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        bot = HackathonBot()
        bot.run()
