import os
import json
import requests
import re
from datetime import datetime
from bs4 import BeautifulSoup

WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')
DB_FILE = "sent_hackathons.txt"

LINKAREER_GRAPHQL_URL = "https://api.linkareer.com/graphql"

LINKAREER_QUERY = """
query FetchActivities($filterBy: ActivityFilter, $page: Int!, $pageSize: Int!) {
  activities(
    filterBy: $filterBy
    pagination: { page: $page, pageSize: $pageSize }
    orderBy: { field: CREATED_AT, direction: DESC }
  ) {
    totalCount
    nodes {
      id
      title
      organizationName
      recruitCloseAt
    }
  }
}
"""

class HackathonBot:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        self.linkareer_headers = {
            **self.headers,
            "Content-Type": "application/json",
            "Origin": "https://linkareer.com",
            "Referer": "https://linkareer.com/",
        }
        self.sent_list = self.load_sent_list()

    def load_sent_list(self):
        if os.path.exists(DB_FILE):
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    def save_sent_list(self, new_items):
        with open(DB_FILE, "a", encoding="utf-8") as f:
            for item in new_items:
                f.write(f"{item['title']}\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆ˜ì§‘ í•¨ìˆ˜ ì„¹ì…˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def fetch_devpost(self):
        try:
            h = self.headers.copy()
            h.update({"Accept": "application/json", "Referer": "https://devpost.com/hackathons", "X-Requested-With": "XMLHttpRequest"})
            res = requests.get("https://devpost.com/api/hackathons", params={"status[]": "upcoming", "sort_by": "Recently Added"}, headers=h, timeout=15)
            if res.status_code == 200:
                return [{"title": h['title'], "url": h['url'], "host": "Devpost", "date": h.get('submission_period_dates', 'N/A')}
                        for h in res.json().get('hackathons', [])]
        except:
            pass
        return []

    def fetch_mlh(self):
        MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}
        try:
            res = requests.get("https://mlh.io/seasons/2026/events", headers=self.headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results, today, seen = [], datetime.now().replace(hour=0,minute=0,second=0,microsecond=0), set()
                for a in soup.find_all('a', href=True):
                    h3 = a.find('h3')
                    if not h3: continue
                    title = h3.get_text(strip=True)
                    if not title or title in seen: continue
                    seen.add(title)
                    link = a['href'].split('?')[0]
                    if not link.startswith('http'): link = "https://mlh.io" + link
                    a_text = a.get_text(separator=' ', strip=True).replace(title, '')
                    date_parts = re.findall(r'([A-Z]{3})\s+(\d{1,2})', a_text)
                    if date_parts:
                        date_str = ' - '.join(f"{m} {d}" for m,d in date_parts) if len(date_parts)>1 else f"{date_parts[0][0]} {date_parts[0][1]}"
                        mon, day = date_parts[-1]
                        end_m = MONTHS.get(mon, 0)
                        if end_m and datetime(today.year, end_m, int(day)) < today: continue
                    else:
                        date_str = "2026 Season"
                    results.append({"title": title, "url": link, "host": "MLH", "date": date_str})
                return results
        except Exception as e:
            print(f"MLH ì˜ˆì™¸: {e}")
        return []

    def _fetch_linkareer(self, filter_by, label):
        """ë§ì»¤ë¦¬ì–´ GraphQL APIë¡œ í™œë™ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        results = []
        page = 1
        page_size = 20
        try:
            while True:
                payload = {
                    "query": LINKAREER_QUERY,
                    "variables": {
                        "filterBy": filter_by,
                        "page": page,
                        "pageSize": page_size,
                    },
                }
                res = requests.post(
                    LINKAREER_GRAPHQL_URL,
                    json=payload,
                    headers=self.linkareer_headers,
                    timeout=15,
                )
                res.raise_for_status()
                data = res.json()
                nodes = data["data"]["activities"]["nodes"]
                total = data["data"]["activities"]["totalCount"]

                for a in nodes:
                    close_date = "ë¯¸ì •"
                    if a.get("recruitCloseAt"):
                        close_ts = int(a["recruitCloseAt"]) / 1000
                        close_date = datetime.fromtimestamp(close_ts).strftime("%Y-%m-%d")
                    results.append({
                        "title": a["title"],
                        "url": f"https://linkareer.com/activity/{a['id']}",
                        "host": f"ë§ì»¤ë¦¬ì–´ | {a.get('organizationName', '-')}",
                        "date": f"ë§ˆê°: {close_date}",
                    })

                if page * page_size >= total:
                    break
                page += 1

        except Exception as e:
            print(f"ë§ì»¤ë¦¬ì–´ {label} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return results

    def fetch_linkareer_hackathon(self):
        """ë§ì»¤ë¦¬ì–´ì—ì„œ í•´ì»¤í†¤ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        return self._fetch_linkareer(
            filter_by={"q": "í•´ì»¤í†¤", "status": "OPEN"},
            label="í•´ì»¤í†¤",
        )

    def fetch_linkareer_bootcamp(self):
        """ë§ì»¤ë¦¬ì–´ì—ì„œ ë¶€íŠ¸ìº í”„ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (êµìœ¡ íƒ€ì…, activityTypeID=6)."""
        return self._fetch_linkareer(
            filter_by={"activityTypeID": 6, "status": "OPEN"},
            label="ë¶€íŠ¸ìº í”„",
        )

    def fetch_campuspick(self):
        try:
            h = self.headers.copy()
            h.update({"Content-Type": "application/x-www-form-urlencoded", "Origin": "https://www2.campuspick.com", "Referer": "https://www2.campuspick.com/"})
            today, results = datetime.now().strftime('%Y-%m-%d'), []
            for cat_id in [108, 111]: # 108: ê³µëª¨ì „, 111: êµìœ¡/ê°•ì—°
                res = requests.post("https://api2.campuspick.com/find/activity/list", data={"target":1,"limit":20,"offset":0,"categoryId":cat_id}, headers=h, timeout=15)
                if res.status_code == 200:
                    activities = res.json().get("result", {}).get("activities", [])
                    for a in activities:
                        if a.get("endDate","") >= today:
                            prefix = "ğŸ“ [ë¶€íŠ¸ìº í”„/êµìœ¡]" if cat_id == 111 else "ğŸ‡°ğŸ‡· [ìº í¼ìŠ¤í”½]"
                            results.append({"title": f"{prefix} {a['title']}", "url": f"https://www2.campuspick.com/contest/view?id={a['id']}", "host": "CampusPick", "date": a.get("endDate","ìƒì„¸ í™•ì¸")})
            return results
        except Exception as e:
            print(f"CampusPick ì˜ˆì™¸: {e}")
        return []

    def fetch_devevent(self):
        try:
            now = datetime.now()
            url = f"https://raw.githubusercontent.com/brave-people/Dev-Event/master/end_event/{now.year}/{str(now.year)[2:]}_{str(now.month).zfill(2)}.md"
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                results = []
                for m in re.finditer(r'__\[([^\]]+)\]\((https?://[^\)]+)\)__', res.text):
                    title, link = m.group(1), m.group(2)
                    target_keywords = ['í•´ì»¤í†¤', 'hackathon', 'ê³µëª¨ì „', 'ê²½ì§„ëŒ€íšŒ', 'ë¶€íŠ¸ìº í”„', 'bootcamp', 'êµìœ¡', 'kdt', 'ì–‘ì„±']
                    if any(k in title.lower() for k in target_keywords):
                        icon = "ğŸ“" if any(b in title.lower() for b in ['ë¶€íŠ¸ìº í”„', 'êµìœ¡', 'kdt']) else "ğŸ‡°ğŸ‡·"
                        results.append({"title": f"{icon} [ë°ë¸Œì´ë²¤íŠ¸] {title}", "url": link, "host": "DevEvent", "date": "ìƒì„¸ í™•ì¸"})
                return results
        except:
            pass
        return []

    def fetch_ssafy(self):
        """SSAFY ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì—ì„œ ëª¨ì§‘ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = "https://www.ssafy.com/ksp/servlet/swp.board.controller.SwpBoardServlet"
            params = {"p_process": "select-board-list", "p_tabseq": "226504", "p_pageno": "1"}
            res = requests.get(url, params=params, headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            results = []
            for row in soup.select('table.tbl-list tbody tr'):
                subj_td = row.select_one('td.subj')
                if not subj_td:
                    continue
                title = subj_td.get_text(strip=True)
                if not any(k in title for k in ['ëª¨ì§‘', 'ê³µê³ ', 'ê¸°ìˆ˜']):
                    continue
                seq_match = re.search(r'goViewPage\((\d+)\)', str(row))
                if not seq_match:
                    continue
                seq = seq_match.group(1)
                detail_url = (
                    f"https://www.ssafy.com/ksp/servlet/swp.board.controller.SwpBoardServlet"
                    f"?p_process=select-board-view&p_tabseq=226504&p_seq={seq}"
                )
                tds = row.find_all('td')
                date = tds[-1].get_text(strip=True) if len(tds) >= 2 else 'ë¯¸ì •'
                results.append({
                    "title": f"[SSAFY] {title}",
                    "url": detail_url,
                    "host": "SSAFY (ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸)",
                    "date": date,
                })
            return results
        except Exception as e:
            print(f"SSAFY ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

    def fetch_woowacourse(self):
        """ìš°ì•„í•œí…Œí¬ì½”ìŠ¤ ê³µì§€ì‚¬í•­ì—ì„œ ëª¨ì§‘ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            res = requests.get("https://woowacourse.io/notice", headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            script = soup.find('script', id='__NEXT_DATA__')
            if not script:
                return []
            blocks = (
                json.loads(script.string)
                .get('props', {})
                .get('pageProps', {})
                .get('recordMap', {})
                .get('block', {})
            )
            results = []
            for block_id, block_data in blocks.items():
                value = block_data.get('value', {})
                if value.get('type') != 'page':
                    continue
                props = value.get('properties', {})
                title_arr = props.get('title', [])
                if not title_arr:
                    continue
                title = title_arr[0][0] if title_arr else ''
                if not title or not any(k in title for k in ['ëª¨ì§‘', 'ì§€ì›', 'ê³¼ì •', 'ê¸°ìˆ˜', 'ì„ ë°œ']):
                    continue
                # ë‚ ì§œ: Notion ì†ì„± í‚¤ê°€ ë™ì ì´ë¯€ë¡œ YYYYë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ìì—´ ê°’ íƒìƒ‰
                date = 'ë¯¸ì •'
                for key, val in props.items():
                    if key == 'title' or not val:
                        continue
                    try:
                        candidate = val[0][0]
                        if isinstance(candidate, str) and re.match(r'\d{4}', candidate):
                            date = candidate[:10]
                            break
                    except (IndexError, TypeError):
                        pass
                results.append({
                    "title": f"[ìš°í…Œì½”] {title}",
                    "url": f"https://woowacourse.io/notice/{block_id}",
                    "host": "ìš°ì•„í•œí…Œí¬ì½”ìŠ¤",
                    "date": date,
                })
            return results
        except Exception as e:
            print(f"ìš°ì•„í•œí…Œí¬ì½”ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

    def fetch_boostcamp(self):
        """ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ ëª¨ì§‘ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        results = []
        pages = [
            ("https://boostcamp.connect.or.kr/guide_ai.html", "AI Tech"),
            ("https://boostcamp.connect.or.kr/main_wm.html", "WebÂ·Mobile"),
        ]
        for url, course in pages:
            try:
                res = requests.get(url, headers=self.headers, timeout=15)
                if res.status_code != 200:
                    continue
                soup = BeautifulSoup(res.text, 'html.parser')
                text = soup.get_text(separator=' ', strip=True)
                # ëª¨ì§‘ ì¤‘ ì—¬ë¶€ í™•ì¸
                recruiting_keywords = ['ëª¨ì§‘ ì¤‘', 'ì§€ì› ê¸°ê°„', 'ëª¨ì§‘ ê¸°ê°„', 'ì ‘ìˆ˜ ê¸°ê°„', 'ëª¨ì§‘í•©ë‹ˆë‹¤', 'ì§€ì›í•˜ê¸°', 'ì›ì„œì ‘ìˆ˜']
                if not any(k in text for k in recruiting_keywords):
                    continue
                # ê¸°ìˆ˜ ì¶”ì¶œ
                cohort_match = re.search(r'(\d+)ê¸°', text)
                cohort = f" {cohort_match.group(1)}ê¸°" if cohort_match else ""
                # ë‚ ì§œ ì¶”ì¶œ
                date_match = re.search(r'(\d{4}[ë…„.\-]\s*\d{1,2}[ì›”.\-]\s*\d{1,2}[ì¼]?)', text)
                date = date_match.group(1).strip() if date_match else 'ìƒì„¸ í™•ì¸'
                results.append({
                    "title": f"[ë¶€ìŠ¤íŠ¸ìº í”„] {course}{cohort} ëª¨ì§‘",
                    "url": url,
                    "host": "ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„",
                    "date": date,
                })
            except Exception as e:
                print(f"ë¶€ìŠ¤íŠ¸ìº í”„ {course} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return results

    def fetch_kt_aivle(self):
        """KT ì—ì´ë¸”ìŠ¤ì¿¨ ê³µì§€ì‚¬í•­ì—ì„œ ëª¨ì§‘ ê³µê³ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = "https://aivle.kt.co.kr/home/brd/bbs/listAtclJson"
            params = {"bbsCd": "NOTICE", "pageIndex": "1"}
            res = requests.get(url, params=params, headers=self.headers, timeout=15)
            res.raise_for_status()
            results = []
            for item in res.json().get("returnList", []):
                title = item.get("atclTitle", "")
                if not any(k in title for k in ['ëª¨ì§‘', 'ê³µê³ ', 'ê¸°ìˆ˜', 'ê³¼ì •', 'ì„ ë°œ']):
                    continue
                seq = item.get("atclSn", "")
                detail_url = (
                    f"https://aivle.kt.co.kr/home/brd/bbs/view?bbsCd=NOTICE&atclSn={seq}"
                    if seq else "https://aivle.kt.co.kr/home/main/goMenuPage?mcd=MC00000061"
                )
                date = item.get("regDttm", "ë¯¸ì •")
                if date and len(date) > 10:
                    date = date[:10]
                results.append({
                    "title": f"[KT ì—ì´ë¸”ìŠ¤ì¿¨] {title}",
                    "url": detail_url,
                    "host": "KT ì—ì´ë¸”ìŠ¤ì¿¨ (AIVLE School)",
                    "date": date,
                })
            return results
        except Exception as e:
            print(f"KT ì—ì´ë¸”ìŠ¤ì¿¨ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìœ í‹¸ë¦¬í‹° ë° ì‹¤í–‰ ì„¹ì…˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def send_to_discord(self, items):
        for i in range(0, len(items), 10):
            chunk = items[i:i+10]
            embeds = [{"title": f"âœ¨ {h['title']}", "url": h['url'], "color": 5814783,
                       "fields": [{"name": "í”Œë«í¼", "value": h['host'], "inline": True},
                                  {"name": "ë§ˆê°/ì¼ì •", "value": str(h['date']), "inline": True}]}
                      for h in chunk]
            requests.post(WEBHOOK_URL, json={
                "content": "ğŸš€ **ìƒˆë¡œìš´ ì†Œì‹ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤!**" if i == 0 else "",
                "embeds": embeds
            })

    def run(self):
        print("ğŸ” í•´ì»¤í†¤ ë° ë¶€íŠ¸ìº í”„ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_items = []
        tasks = [
            ("Devpost", self.fetch_devpost),
            ("MLH", self.fetch_mlh),
            ("DevEvent", self.fetch_devevent),
            ("CampusPick", self.fetch_campuspick),
            ("ë§ì»¤ë¦¬ì–´ í•´ì»¤í†¤", self.fetch_linkareer_hackathon),
            ("ë§ì»¤ë¦¬ì–´ ë¶€íŠ¸ìº í”„", self.fetch_linkareer_bootcamp),
            ("SSAFY", self.fetch_ssafy),
            ("ìš°ì•„í•œí…Œí¬ì½”ìŠ¤", self.fetch_woowacourse),
            ("ë¶€ìŠ¤íŠ¸ìº í”„", self.fetch_boostcamp),
            ("KT ì—ì´ë¸”ìŠ¤ì¿¨", self.fetch_kt_aivle),
        ]

        for name, fetcher in tasks:
            try:
                found = fetcher()
                print(f"ğŸ“¡ {name}: {len(found)}ê°œ ë°œê²¬")
                all_items.extend(found)
            except Exception as e:
                print(f"âŒ {name} ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€) ë° ì‹ ê·œ í•­ëª© í•„í„°ë§
        seen_titles, deduped = set(), []
        for item in all_items:
            if item['title'] not in seen_titles:
                seen_titles.add(item['title'])
                deduped.append(item)

        new_items = [i for i in deduped if i['title'] not in self.sent_list]
        print(f"ğŸ“Š ìµœì¢… ì‹ ê·œ ê³µê³ : {len(new_items)}ê°œ")

        if new_items:
            self.send_to_discord(new_items)
            self.save_sent_list(new_items)

if __name__ == "__main__":
    if WEBHOOK_URL:
        HackathonBot().run()
    else:
        print("âŒ DISCORD_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
