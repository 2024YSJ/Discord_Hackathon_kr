import os
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time
import re
# ì„¤ì •
WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_URL')
DB_FILE = "sent_hackathons.txt"

class HackathonBot:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # Kaggle ì „ìš© í—¤ë” (base_headers ëŒ€ìš©)
        self.base_headers = self.headers.copy()
        self.sent_list = self.load_sent_list()

    def load_sent_list(self):
        if os.path.exists(DB_FILE):
            with open(DB_FILE, "r", encoding="utf-8") as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    def save_sent_list(self, new_items):
        with open(DB_FILE, "a", encoding="utf-8") as f:
            for item in new_items:
                f.write(f"{item['title']}\n")

    # --- í”Œë«í¼ë³„ í¬ë¡¤ëŸ¬/API í˜¸ì¶œ ë©”ì„œë“œ (í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ ë“¤ì—¬ì“°ê¸° ì™„ë£Œ) ---

    def fetch_devpost(self):
        try:
            custom_headers = self.headers.copy()
            custom_headers.update({
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Referer": "https://devpost.com/hackathons",
                "X-Requested-With": "XMLHttpRequest"
            })
            params = {"status[]": "upcoming", "sort_by": "Recently Added"}
            url = "https://devpost.com/api/hackathons"
            res = requests.get(url, params=params, headers=custom_headers, timeout=15)
            if res.status_code == 200:
                data = res.json()
                hackathons = data.get('hackathons', [])
                return [{"title": h['title'], "url": h['url'], "host": "Devpost", "date": h.get('submission_period_dates', 'N/A')} for h in hackathons]
            return []
        except: return []

    def fetch_mlh(self):
        """MLH 2026 ì‹œì¦Œ í˜ì´ì§€ í¬ë¡¤ë§ - ë¯¸ë˜ ì´ë²¤íŠ¸ë§Œ ë°˜í™˜"""
        MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,
                  'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}
        try:
            url = "https://mlh.io/seasons/2026/events"
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results = []
                today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                seen = set()
                for a in soup.find_all('a', href=True):
                    h3 = a.find('h3')
                    if not h3:
                        continue
                    title = h3.get_text(strip=True)
                    if not title or title in seen:
                        continue
                    seen.add(title)
                    link = a['href'].split('?')[0]
                    if not link.startswith('http'):
                        link = "https://mlh.io" + link
                    # <a> ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: "FEB 27", "MAR 01")
                    a_text = a.get_text(separator=' ', strip=True).replace(title, '')
                    date_parts = re.findall(r'([A-Z]{3})\s+(\d{1,2})', a_text)
                    if date_parts:
                        date_str = ' - '.join(f"{m} {d}" for m, d in date_parts) if len(date_parts) > 1 else f"{date_parts[0][0]} {date_parts[0][1]}"
                        # ì¢…ë£Œì¼(ë§ˆì§€ë§‰ ë‚ ì§œ)ì´ ì˜¤ëŠ˜ ì´ì „ì´ë©´ ìŠ¤í‚µ
                        mon, day = date_parts[-1]
                        end_m = MONTHS.get(mon, 0)
                        if end_m:
                            event_end = datetime(today.year, end_m, int(day))
                            if event_end < today:
                                continue
                    else:
                        date_str = "2026 Season"
                    results.append({
                        "title": title,
                        "url": link,
                        "host": "MLH",
                        "date": date_str
                    })
                print(f"ğŸ“¡ MLH: {len(results)}ê°œ ì¶”ì¶œ ì„±ê³µ (ì¢…ë£Œ ì´ë²¤íŠ¸ ì œì™¸)")
                return results
            else:
                print(f"MLH ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}")
        except Exception as e:
            print(f"MLH í¬ë¡¤ë§ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

    def fetch_devfolio(self):
        try:
            dev_headers = self.headers.copy()
            dev_headers.update({"Origin": "https://devfolio.co", "Referer": "https://devfolio.co/hackathons"})
            today = datetime.now().strftime('%Y-%m-%d')
            res = requests.get(
                "https://api.devfolio.co/api/hackathons",
                params={"type": "open", "limit": 20, "page": 1},
                headers=dev_headers, timeout=15
            )
            if res.status_code == 200:
                return [{"title": h.get('name'), "url": f"https://{h.get('slug')}.devfolio.co",
                         "host": "Devfolio", "date": (h.get('ends_at') or 'N/A')[:10]}
                        for h in res.json().get('result', [])
                        if h.get('slug') and (h.get('ends_at') or '9999') >= today]
            return []
        except: return []

    def fetch_dorahacks(self):
        # DoraHacksëŠ” __NEXT_DATA__ ë°©ì‹ì—ì„œ Nuxt.js í´ë¼ì´ì–¸íŠ¸ ë Œë”ë§ìœ¼ë¡œ ì „í™˜ë¨
        # ê³µì‹ API íƒìƒ‰
        try:
            for api_url in [
                "https://dorahacks.io/api/hackathon?status=open&limit=20",
                "https://dorahacks.io/api/v1/hackathon?status=upcoming&size=20",
            ]:
                res = requests.get(api_url, headers=self.headers, timeout=10)
                if res.status_code == 200:
                    try:
                        data = res.json()
                        items = data if isinstance(data, list) else data.get('data', data.get('list', []))
                        if isinstance(items, list) and items:
                            return [{"title": h.get('title') or h.get('name', ''),
                                     "url": f"https://dorahacks.io/hackathon/{h.get('id', '')}",
                                     "host": "DoraHacks", "date": "ìƒì„¸ í™•ì¸"}
                                    for h in items if h.get('title') or h.get('name')]
                    except Exception:
                        pass
        except Exception as e:
            print(f"DoraHacks í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_unstop(self):
        try:
            url = "https://unstop.com/api/public/opportunity/search-result?opportunity=hackathons&per_page=15"
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code == 200:
                items = res.json().get('data', {}).get('data', [])
                return [{"title": h.get('title'), "url": f"https://unstop.com/p/{h.get('public_url')}", "host": "Unstop", "date": h.get('reg_end_date', 'N/A').split('T')[0]} for h in items if h.get('public_url')]
            return []
        except: return []

    def fetch_kaggle(self):
        # Kaggleì€ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë Œë”ë§ìœ¼ë¡œ window.Kaggle.Stateê°€ ë” ì´ìƒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
        # __NEXT_DATA__ ë˜ëŠ” JSON-LD ë°©ì‹ ì‹œë„
        try:
            url = "https://www.kaggle.com/competitions?hostSegmentIdFilter=8"
            res = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            # Next.js ë°ì´í„° ì‹œë„
            script = soup.find('script', id='__NEXT_DATA__')
            if script:
                data = json.loads(script.string)
                items = data.get('props', {}).get('pageProps', {}).get('competitions', [])
                return [{"title": i['title'], "url": f"https://www.kaggle.com/c/{i.get('ref', i.get('id', ''))}", "host": "Kaggle", "date": i.get('deadline', 'N/A')} for i in items if i.get('title')]
            # JSON-LD êµ¬ì¡°í™” ë°ì´í„° ì‹œë„
            for s in soup.find_all('script', type='application/ld+json'):
                try:
                    ld = json.loads(s.string)
                    if isinstance(ld, list):
                        return [{"title": e.get('name', ''), "url": e.get('url', ''), "host": "Kaggle", "date": e.get('endDate', 'N/A')} for e in ld if e.get('name')]
                except: continue
        except: pass
        return []

    def fetch_hack2skill(self):
        # api.hack2skill.com ë„ë©”ì¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ â†’ ëŒ€ì²´ ì—”ë“œí¬ì¸íŠ¸ ì‹œë„
        try:
            for url in [
                "https://hack2skill.com/api/v1/hackathons?status=upcoming",
                "https://hack2skill.com/api/hackathons",
            ]:
                res = requests.get(url, headers=self.headers, timeout=10)
                if res.status_code == 200:
                    try:
                        items = res.json()
                        if isinstance(items, dict):
                            items = items.get('data', items.get('hackathons', []))
                        if isinstance(items, list) and items and isinstance(items[0], dict):
                            return [{"title": h.get('name') or h.get('title', ''),
                                     "url": f"https://hack2skill.com/hackathon/{h.get('slug', h.get('id', ''))}",
                                     "host": "Hack2Skill",
                                     "date": (h.get('start_date') or h.get('startDate') or 'N/A')[:10]}
                                    for h in items if h.get('slug') or h.get('id')]
                    except Exception:
                        pass
        except Exception as e:
            print(f"Hack2Skill í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_programmers(self):
        try:
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹Œ ì „ì²´ ì±Œë¦°ì§€ í˜ì´ì§€
            url = "https://programmers.co.kr/learn/challenges"
            res = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            results = []
            # 'challenge-card' í´ë˜ìŠ¤ ì™¸ì— ì œëª©ì„ í¬í•¨í•˜ëŠ” ëª¨ë“  ë§í¬ íƒìƒ‰
            for a in soup.select('a[href*="/learn/challenges/"]'):
                title_el = a.select_one('h4, .title, h5')
                if title_el:
                    title = title_el.get_text(strip=True)
                    if any(k in title for k in ['í•´ì»¤í†¤', 'ì±Œë¦°ì§€', 'ëŒ€íšŒ']):
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [í”„ë¡œê·¸ë˜ë¨¸ìŠ¤] {title}",
                            "url": "https://programmers.co.kr" + a['href'],
                            "host": "Programmers", "date": "ìƒì„¸ í™•ì¸"
                        })
            return results
        except: pass
        return []

    def fetch_devevent(self):
        """brave-people/Dev-Event ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹± (í•œêµ­ ê°œë°œ ì´ë²¤íŠ¸)"""
        try:
            now = datetime.now()
            year_short = str(now.year)[2:]   # ì˜ˆ: "26"
            month = str(now.month).zfill(2)  # ì˜ˆ: "02"
            url = f"https://raw.githubusercontent.com/brave-people/Dev-Event/master/end_event/{now.year}/{year_short}_{month}.md"
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                results = []
                # ë§ˆí¬ë‹¤ìš´ í˜•ì‹: - __[ì œëª©](URL)__
                for m in re.finditer(r'__\[([^\]]+)\]\((https?://[^\)]+)\)__', res.text):
                    title, link = m.group(1), m.group(2)
                    if any(k in title for k in ['í•´ì»¤í†¤', 'Hackathon', 'hackathon', 'ê³µëª¨ì „', 'ê²½ì§„ëŒ€íšŒ']):
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [ë°ë¸Œì´ë²¤íŠ¸] {title}",
                            "url": link,
                            "host": "DevEvent",
                            "date": "ìƒì„¸ í™•ì¸"
                        })
                return results
        except Exception as e:
            print(f"DevEvent í¬ë¡¤ë§ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

    def fetch_goorm(self):
        try:
            headers = self.headers.copy()
            headers.update({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            })
            url = "https://level.goorm.io/l/challenge"
            res = requests.get(url, headers=headers, timeout=15)
            if res.status_code != 200:
                return []
            soup = BeautifulSoup(res.text, 'html.parser')
            results = []
            seen = set()
            for item in soup.find_all(['div', 'a'], class_=re.compile(r'card|item|challenge|contest')):
                title_el = item.find(['h3', 'h4', 'h2', 'div', 'span'], class_=re.compile(r'title|name|subject'))
                if not title_el:
                    continue
                title = title_el.get_text(strip=True)
                if not title or title in seen:
                    continue
                seen.add(title)
                link_el = item if item.name == 'a' else item.find('a')
                if link_el and link_el.get('href'):
                    href = link_el['href']
                    full_url = href if href.startswith('http') else "https://level.goorm.io" + href
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [êµ¬ë¦„] {title}",
                        "url": full_url,
                        "host": "goorm",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
            return results
        except Exception as e:
            print(f"Goorm í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_wevity(self):
        """ìœ„ë¹„í‹° í•´ì»¤í†¤ ê³µëª¨ì „ ëª©ë¡ íŒŒì‹± (ì„œë²„ì‚¬ì´ë“œ ë Œë”ë§)"""
        try:
            # params ì‚¬ìš©ìœ¼ë¡œ í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€
            res = requests.get(
                "https://www.wevity.com/",
                params={'c': 'find', 's': '1', 'sp': 'contents', 'sw': 'í•´ì»¤í†¤'},
                headers=self.headers, timeout=15
            )
            print(f"  Wevity HTTP {res.status_code}, {len(res.text)} bytes")
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                ul = soup.find('ul', class_='list')
                if not ul:
                    print("  Wevity: ul.list ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í•¨")
                    return []
                results = []
                li_all = ul.find_all('li')
                print(f"  Wevity: {len(li_all)}ê°œ li ë°œê²¬")
                for li in li_all:
                    if 'top' in li.get('class', []):
                        continue
                    # dday span í…ìŠ¤íŠ¸ë¡œ ë§ˆê° ì—¬ë¶€ í™•ì¸ ('ë§ˆê°ì„ë°•'ì€ í¬í•¨)
                    dday_span = li.find('span', class_='dday')
                    if dday_span and dday_span.get_text(strip=True) == 'ë§ˆê°':
                        continue
                    tit_div = li.find('div', class_='tit')
                    if not tit_div:
                        continue
                    a = tit_div.find('a', href=True)
                    if not a:
                        continue
                    title = a.get_text(strip=True)
                    if not title:
                        continue
                    href = a['href']
                    full_url = "https://www.wevity.com/" + href if href.startswith('?') else href
                    day_div = li.find('div', class_='day')
                    date_str = day_div.get_text(separator=' ', strip=True) if day_div else "ìƒì„¸ í™•ì¸"
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [ìœ„ë¹„í‹°] {title}",
                        "url": full_url,
                        "host": "Wevity",
                        "date": date_str
                    })
                return results
        except Exception as e:
            print(f"Wevity í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_campuspick(self):
        """ìº í¼ìŠ¤í”½ í•´ì»¤í†¤ ê³µëª¨ì „ ëª©ë¡"""
        try:
            headers = self.headers.copy()
            headers.update({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
                "Referer": "https://www.campuspick.com/",
            })
            # campuspick.comì€ www2ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨
            url = "https://www2.campuspick.com/contest?category=108&keyword=%ED%95%B4%EC%BB%A4%ED%86%A4"
            res = requests.get(url, headers=headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results = []
                # ê³µëª¨ì „ ë§í¬ íŒ¨í„´ìœ¼ë¡œ í•­ëª© íƒìƒ‰
                for a in soup.find_all('a', href=re.compile(r'/contest/\d+')):
                    title_el = a.find(['h3', 'h4', 'h2', 'p', 'span'],
                                       class_=re.compile(r'title|name|tit|subject'))
                    title = title_el.get_text(strip=True) if title_el else a.get_text(strip=True)
                    if not title:
                        continue
                    href = a['href']
                    full_url = "https://www2.campuspick.com" + href if href.startswith('/') else href
                    results.append({
                        "title": f"ğŸ‡°ğŸ‡· [ìº í¼ìŠ¤í”½] {title}",
                        "url": full_url,
                        "host": "CampusPick",
                        "date": "ìƒì„¸ í™•ì¸"
                    })
                return results
        except Exception as e:
            print(f"CampusPick í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_aiconnect(self):
        """AI Connect ëŒ€íšŒ ëª©ë¡ (Nuxt.js, window.__NUXT__ ë°ì´í„° íƒìƒ‰)"""
        try:
            headers = self.headers.copy()
            headers.update({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9",
                "Referer": "https://aiconnect.kr/",
            })
            res = requests.get("https://aiconnect.kr/competition/list", headers=headers, timeout=15)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                results = []
                for script in soup.find_all('script'):
                    text = script.get_text()
                    if '__NUXT__' not in text:
                        continue
                    m = re.search(r'window\.__NUXT__\s*=\s*(\{.+\})\s*;?\s*$', text, re.DOTALL)
                    if not m:
                        continue
                    try:
                        nuxt = json.loads(m.group(1))
                        # ê°€ëŠ¥í•œ ê²½ë¡œë“¤ ìˆœì„œëŒ€ë¡œ íƒìƒ‰
                        candidates = [
                            nuxt.get('state', {}).get('competitions', []),
                            nuxt.get('state', {}).get('items', []),
                            nuxt.get('data', [{}])[0].get('competitions', []) if nuxt.get('data') else [],
                        ]
                        for comps in candidates:
                            if not isinstance(comps, list) or not comps:
                                continue
                            for c in comps:
                                if not isinstance(c, dict):
                                    continue
                                title = c.get('title') or c.get('name', '')
                                cid = c.get('id') or c.get('competitionId', '')
                                if title:
                                    results.append({
                                        "title": f"ğŸ‡°ğŸ‡· [AI Connect] {title}",
                                        "url": f"https://aiconnect.kr/competition/detail/{cid}",
                                        "host": "AIConnect",
                                        "date": c.get('endDate', 'ìƒì„¸ í™•ì¸')
                                    })
                            break
                    except json.JSONDecodeError:
                        pass
                return results
        except Exception as e:
            print(f"AIConnect í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def fetch_linkareer(self):
        """ë§ì»¤ë¦¬ì–´ GraphQL API - í•´ì»¤í†¤ í™œë™ ê²€ìƒ‰"""
        try:
            query = "{ activities { nodes { id title organizationName categories { name } createdAt } } }"
            res = requests.post(
                "https://api.linkareer.com/graphql",
                json={"query": query},
                headers={"Content-Type": "application/json",
                         "User-Agent": self.headers["User-Agent"]},
                timeout=15
            )
            if res.status_code == 200:
                nodes = res.json().get('data', {}).get('activities', {}).get('nodes', [])
                results = []
                for node in nodes:
                    title = node.get('title', '')
                    cats = ' '.join(c.get('name', '') for c in (node.get('categories') or []))
                    if any(k in title + cats for k in ['í•´ì»¤í†¤', 'Hackathon', 'hackathon']):
                        nid = node.get('id', '')
                        results.append({
                            "title": f"ğŸ‡°ğŸ‡· [ë§ì»¤ë¦¬ì–´] {title}",
                            "url": f"https://linkareer.com/activity/{nid}",
                            "host": "Linkareer",
                            "date": "ìƒì„¸ í™•ì¸"
                        })
                return results
        except Exception as e:
            print(f"Linkareer í¬ë¡¤ë§ ì˜ˆì™¸: {e}")
        return []

    def run(self):
        print("ğŸ” í•´ì»¤í†¤ ì •ë³´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_hackathons = []

        # í•¨ìˆ˜ ëª©ë¡ê³¼ ì´ë¦„ ë§¤í•‘
        tasks = [
            ("Devpost", self.fetch_devpost),
            ("MLH", self.fetch_mlh),
            ("Devfolio", self.fetch_devfolio),
            ("Unstop", self.fetch_unstop),
            ("Kaggle", self.fetch_kaggle),
            ("Hack2Skill", self.fetch_hack2skill),
            ("DoraHacks", self.fetch_dorahacks),
            ("Programmers", self.fetch_programmers),
            ("DevEvent", self.fetch_devevent),
            ("goorm", self.fetch_goorm),
            ("Wevity", self.fetch_wevity),
            ("CampusPick", self.fetch_campuspick),
            ("AIConnect", self.fetch_aiconnect),
            ("Linkareer", self.fetch_linkareer),
        ]
        
        for name, fetcher in tasks:
            try:
                found = fetcher()
                print(f"ğŸ“¡ {name}: {len(found)}ê°œ ë°œê²¬") # ë¡œê·¸ ì¶œë ¥
                all_hackathons.extend(found)
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±°
        new_items = [h for h in all_hackathons if h['title'] not in self.sent_list]
        print(f"ğŸ“Š ìµœì¢… ì‹ ê·œ ê³µê³ : {len(new_items)}ê°œ")

        if not new_items:
            return

        self.send_to_discord(new_items)
        self.save_sent_list(new_items)

    def send_to_discord(self, hackathons):
        for i in range(0, len(hackathons), 10):
            chunk = hackathons[i:i+10]
            embeds = []
            for h in chunk:
                embeds.append({
                    "title": f"ğŸ† {h['title']}",
                    "url": h['url'],
                    "color": 3447003,
                    "fields": [
                        {"name": "í”Œë«í¼", "value": h['host'], "inline": True},
                        {"name": "ë§ˆê°/ì¼ì •", "value": str(h['date']), "inline": True}
                    ]
                })
            payload = {
                "content": "ğŸš€ **ìƒˆë¡œìš´ í•´ì»¤í†¤ ëŒ€íšŒê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!**" if i == 0 else "",
                "embeds": embeds
            }
            requests.post(WEBHOOK_URL, json=payload)

if __name__ == "__main__":
    if not WEBHOOK_URL:
        print("âŒ ì˜¤ë¥˜: DISCORD_WEBHOOK_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        bot = HackathonBot()
        bot.run()
